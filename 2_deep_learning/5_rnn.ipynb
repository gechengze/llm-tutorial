{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b012d8-fada-4ad4-aac3-58e85b2b6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import collections\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90a0a6-a8a4-416b-9cb0-ac6a440f5ec5",
   "metadata": {},
   "source": [
    "# 1. 概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d85131-69e0-4cdd-801e-1e342d1663f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "想象一下有人正在看网飞（Netflix，一个国外的视频网站）上的电影。\n",
    "一名忠实的用户会对每一部电影都给出评价，\n",
    "毕竟一部好电影需要更多的支持和认可。\n",
    "然而事实证明，事情并不那么简单。\n",
    "随着时间的推移，人们对电影的看法会发生很大的变化。\n",
    "事实上，心理学家甚至对这些现象起了名字：\n",
    "\n",
    "* *锚定*（anchoring）效应：基于其他人的意见做出评价。\n",
    "  例如，奥斯卡颁奖后，受到关注的电影的评分会上升，尽管它还是原来那部电影。\n",
    "  这种影响将持续几个月，直到人们忘记了这部电影曾经获得的奖项。\n",
    "  结果表明这种效应会使评分提高半个百分点以上。\n",
    "* *享乐适应*（hedonic adaption）：人们迅速接受并且适应一种更好或者更坏的情况\n",
    "  作为新的常态。\n",
    "  例如，在看了很多好电影之后，人们会强烈期望下部电影会更好。\n",
    "  因此，在许多精彩的电影被看过之后，即使是一部普通的也可能被认为是糟糕的。\n",
    "* *季节性*（seasonality）：少有观众喜欢在八月看圣诞老人的电影。\n",
    "* 有时，电影会由于导演或演员在制作中的不当行为变得不受欢迎。\n",
    "* 有些电影因为其极度糟糕只能成为小众电影。*Plan9from Outer Space*和*Troll2*就因为这个原因而臭名昭著的。\n",
    "\n",
    "简而言之，电影评分决不是固定不变的。\n",
    "因此，使用时间动力学可以得到更准确的电影推荐。\n",
    "当然，序列数据不仅仅是关于电影评分的。\n",
    "下面给出了更多的场景。\n",
    "\n",
    "* 在使用程序时，许多用户都有很强的特定习惯。\n",
    "  例如，在学生放学后社交媒体应用更受欢迎。在市场开放时股市交易软件更常用。\n",
    "* 预测明天的股价要比过去的股价更困难，尽管两者都只是估计一个数字。\n",
    "  毕竟，先见之明比事后诸葛亮难得多。\n",
    "  在统计学中，前者（对超出已知观测范围进行预测）称为*外推法*（extrapolation），\n",
    "  而后者（在现有观测值之间进行估计）称为*内插法*（interpolation）。\n",
    "* 在本质上，音乐、语音、文本和视频都是连续的。\n",
    "  如果它们的序列被我们重排，那么就会失去原有的意义。\n",
    "  比如，一个文本标题“狗咬人”远没有“人咬狗”那么令人惊讶，尽管组成两句话的字完全相同。\n",
    "* 地震具有很强的相关性，即大地震发生后，很可能会有几次小余震，\n",
    "  这些余震的强度比非大地震后的余震要大得多。\n",
    "  事实上，地震是时空相关的，即余震通常发生在很短的时间跨度和很近的距离内。\n",
    "* 人类之间的互动也是连续的，这可以从微博上的争吵和辩论中看出。\n",
    "\n",
    "**统计工具**\n",
    "\n",
    "处理序列数据需要统计工具和新的深度神经网络架构。\n",
    "为了简单起见，以下图所示的股票价格（富时100指数）为例。\n",
    "\n",
    "![近30年的富时100指数](../img/ftse100.png)\n",
    "\n",
    "其中，用$x_t$表示价格，即在*时间步*（time step）\n",
    "$t \\in \\mathbb{Z}^+$时，观察到的价格$x_t$。\n",
    "请注意，$t$对于本文中的序列通常是离散的，并在整数或其子集上变化。\n",
    "假设一个交易员想在$t$日的股市中表现良好，于是通过以下途径预测$x_t$：\n",
    "\n",
    "$$x_t \\sim P(x_t \\mid x_{t-1}, \\ldots, x_1).$$\n",
    "\n",
    "**自回归模型**\n",
    "\n",
    "为了实现这个预测，交易员可以使用回归模型，比如说线性回归。\n",
    "仅有一个主要问题：输入数据的数量，\n",
    "输入$x_{t-1}, \\ldots, x_1$本身因$t$而异。\n",
    "也就是说，输入数据的数量这个数字将会随着我们遇到的数据量的增加而增加，\n",
    "因此需要一个近似方法来使这个计算变得容易处理。\n",
    "本章后面的大部分内容将围绕着如何有效估计\n",
    "$P(x_t \\mid x_{t-1}, \\ldots, x_1)$展开。\n",
    "简单地说，它归结为以下两种策略。\n",
    "\n",
    "第一种策略，假设在现实情况下相当长的序列\n",
    "$x_{t-1}, \\ldots, x_1$可能是不必要的，\n",
    "因此我们只需要满足某个长度为$\\tau$的时间跨度，\n",
    "即使用观测序列$x_{t-1}, \\ldots, x_{t-\\tau}$。\n",
    "当下获得的最直接的好处就是参数的数量总是不变的，\n",
    "至少在$t > \\tau$时如此，这就使我们能够训练一个上面提及的深度网络。\n",
    "这种模型被称为*自回归模型*（autoregressive models），\n",
    "因为它们是对自己执行回归。\n",
    "\n",
    "第二种策略，如下图所示，\n",
    "是保留一些对过去观测的总结$h_t$，\n",
    "并且同时更新预测$\\hat{x}_t$和总结$h_t$。\n",
    "这就产生了基于$\\hat{x}_t = P(x_t \\mid h_{t})$估计$x_t$，\n",
    "以及公式$h_t = g(h_{t-1}, x_{t-1})$更新的模型。\n",
    "由于$h_t$从未被观测到，这类模型也被称为\n",
    "*隐变量自回归模型*（latent autoregressive models）。\n",
    "\n",
    "![隐变量自回归模型](../img/sequence-model.svg)\n",
    "\n",
    "这两种情况都有一个显而易见的问题：如何生成训练数据？\n",
    "一个经典方法是使用历史观测来预测下一个未来观测。\n",
    "显然，我们并不指望时间会停滞不前。\n",
    "然而，一个常见的假设是虽然特定值$x_t$可能会改变，\n",
    "但是序列本身的动力学不会改变。\n",
    "这样的假设是合理的，因为新的动力学一定受新的数据影响，\n",
    "而我们不可能用目前所掌握的数据来预测新的动力学。\n",
    "统计学家称不变的动力学为*静止的*（stationary）。\n",
    "因此，整个序列的估计值都将通过以下的方式获得：\n",
    "\n",
    "$$P(x_1, \\ldots, x_T) = \\prod_{t=1}^T P(x_t \\mid x_{t-1}, \\ldots, x_1).$$\n",
    "\n",
    "注意，如果我们处理的是离散的对象（如单词），\n",
    "而不是连续的数字，则上述的考虑仍然有效。\n",
    "唯一的差别是，对于离散的对象，\n",
    "我们需要使用分类器而不是回归模型来估计$P(x_t \\mid  x_{t-1}, \\ldots, x_1)$。\n",
    "\n",
    "**马尔可夫模型**\n",
    "\n",
    "回想一下，在自回归模型的近似法中，\n",
    "我们使用$x_{t-1}, \\ldots, x_{t-\\tau}$\n",
    "而不是$x_{t-1}, \\ldots, x_1$来估计$x_t$。\n",
    "只要这种是近似精确的，我们就说序列满足*马尔可夫条件*（Markov condition）。\n",
    "特别是，如果$\\tau = 1$，得到一个\n",
    "*一阶马尔可夫模型*（first-order Markov model），\n",
    "$P(x)$由下式给出：\n",
    "\n",
    "$$P(x_1, \\ldots, x_T) = \\prod_{t=1}^T P(x_t \\mid x_{t-1}) \\text{ 当 } P(x_1 \\mid x_0) = P(x_1).$$\n",
    "\n",
    "当假设$x_t$仅是离散值时，这样的模型特别棒，\n",
    "因为在这种情况下，使用动态规划可以沿着马尔可夫链精确地计算结果。\n",
    "例如，我们可以高效地计算$P(x_{t+1} \\mid x_{t-1})$：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(x_{t+1} \\mid x_{t-1})\n",
    "&= \\frac{\\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\\\\\n",
    "&= \\frac{\\sum_{x_t} P(x_{t+1} \\mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\\\\\n",
    "&= \\sum_{x_t} P(x_{t+1} \\mid x_t) P(x_t \\mid x_{t-1})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "利用这一事实，我们只需要考虑过去观察中的一个非常短的历史：\n",
    "$P(x_{t+1} \\mid x_t, x_{t-1}) = P(x_{t+1} \\mid x_t)$。\n",
    "\n",
    "**因果关系**\n",
    "\n",
    "原则上，将$P(x_1, \\ldots, x_T)$倒序展开也没什么问题。\n",
    "毕竟，基于条件概率公式，我们总是可以写出：\n",
    "\n",
    "$$P(x_1, \\ldots, x_T) = \\prod_{t=T}^1 P(x_t \\mid x_{t+1}, \\ldots, x_T).$$\n",
    "\n",
    "事实上，如果基于一个马尔可夫模型，\n",
    "我们还可以得到一个反向的条件概率分布。\n",
    "然而，在许多情况下，数据存在一个自然的方向，即在时间上是前进的。\n",
    "很明显，未来的事件不能影响过去。\n",
    "因此，如果我们改变$x_t$，可能会影响未来发生的事情$x_{t+1}$，但不能反过来。\n",
    "也就是说，如果我们改变$x_t$，基于过去事件得到的分布不会改变。\n",
    "因此，解释$P(x_{t+1} \\mid x_t)$应该比解释$P(x_t \\mid x_{t+1})$更容易。\n",
    "例如，在某些情况下，对于某些可加性噪声$\\epsilon$，\n",
    "显然我们可以找到$x_{t+1} = f(x_t) + \\epsilon$，\n",
    "而反之则不行 :cite:`Hoyer.Janzing.Mooij.ea.2009`。\n",
    "而这个向前推进的方向恰好也是我们通常感兴趣的方向。\n",
    "彼得斯等人 :cite:`Peters.Janzing.Scholkopf.2017`\n",
    "对该主题的更多内容做了详尽的解释，而我们的上述讨论只是其中的冰山一角。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a995a56-444d-4096-a59f-d7cee71d77ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 2. 简单的示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecee87-9358-4e17-857a-229b67114f5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2.1 数据构造"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785402eb-1978-42ec-9cf2-a020cce54eb7",
   "metadata": {},
   "source": [
    "在了解了上述统计工具后，让我们在实践中尝试一下！\n",
    "首先，我们生成一些数据：(**使用正弦函数和一些可加性噪声来生成序列数据，\n",
    "时间步为$1, 2, \\ldots, 1000$。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca12cb0-fda3-45c0-8a68-763ee7a037fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"406.885938pt\" height=\"211.07625pt\" viewBox=\"0 0 406.885938 211.07625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-02-10T19:12:33.845286</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 211.07625 \n",
       "L 406.885938 211.07625 \n",
       "L 406.885938 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 173.52 \n",
       "L 386.960938 173.52 \n",
       "L 386.960938 7.2 \n",
       "L 52.160938 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 118.852829 173.52 \n",
       "L 118.852829 7.2 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m46361cba77\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m46361cba77\" x=\"118.852829\" y=\"173.52\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(109.309079 188.118438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 185.879856 173.52 \n",
       "L 185.879856 7.2 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m46361cba77\" x=\"185.879856\" y=\"173.52\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(176.336106 188.118438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 252.906883 173.52 \n",
       "L 252.906883 7.2 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m46361cba77\" x=\"252.906883\" y=\"173.52\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(243.363133 188.118438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 319.93391 173.52 \n",
       "L 319.93391 7.2 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m46361cba77\" x=\"319.93391\" y=\"173.52\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 800 -->\n",
       "      <g transform=\"translate(310.39016 188.118438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 386.960938 173.52 \n",
       "L 386.960938 7.2 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m46361cba77\" x=\"386.960938\" y=\"173.52\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(374.235937 188.118438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- time -->\n",
       "     <g transform=\"translate(208.264844 201.796563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"66.992188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"164.404297\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 52.160938 167.050079 \n",
       "L 386.960938 167.050079 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"m318847067e\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m318847067e\" x=\"52.160938\" y=\"167.050079\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −1.5 -->\n",
       "      <g transform=\"translate(20.878125 170.849298) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 52.160938 141.222239 \n",
       "L 386.960938 141.222239 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m318847067e\" x=\"52.160938\" y=\"141.222239\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- −1.0 -->\n",
       "      <g transform=\"translate(20.878125 145.021458) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 52.160938 115.394399 \n",
       "L 386.960938 115.394399 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m318847067e\" x=\"52.160938\" y=\"115.394399\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 119.193618) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 52.160938 89.56656 \n",
       "L 386.960938 89.56656 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m318847067e\" x=\"52.160938\" y=\"89.56656\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 93.365778) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 52.160938 63.73872 \n",
       "L 386.960938 63.73872 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m318847067e\" x=\"52.160938\" y=\"63.73872\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 67.537939) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 52.160938 37.91088 \n",
       "L 386.960938 37.91088 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m318847067e\" x=\"52.160938\" y=\"37.91088\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(29.257812 41.710099) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 52.160938 12.083041 \n",
       "L 386.960938 12.083041 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m318847067e\" x=\"52.160938\" y=\"12.083041\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 1.5 -->\n",
       "      <g transform=\"translate(29.257812 15.882259) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(14.798438 93.319375) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_25\">\n",
       "    <path d=\"M 52.160938 77.143502 \n",
       "L 52.496073 85.029157 \n",
       "L 52.831208 78.528442 \n",
       "L 53.166343 80.137507 \n",
       "L 53.501478 102.100134 \n",
       "L 53.836613 94.550214 \n",
       "L 54.171748 70.092585 \n",
       "L 54.506883 70.631558 \n",
       "L 54.842019 95.352999 \n",
       "L 55.177154 96.389347 \n",
       "L 55.512289 92.133625 \n",
       "L 55.847424 64.15594 \n",
       "L 56.182559 92.250444 \n",
       "L 56.517694 91.30947 \n",
       "L 56.852829 94.265246 \n",
       "L 57.187965 79.876932 \n",
       "L 57.5231 70.442603 \n",
       "L 57.858235 79.824866 \n",
       "L 58.19337 73.004245 \n",
       "L 58.528505 87.870902 \n",
       "L 58.86364 89.5073 \n",
       "L 59.198775 71.242133 \n",
       "L 59.53391 66.954595 \n",
       "L 59.869046 86.425076 \n",
       "L 60.204181 77.455458 \n",
       "L 60.539316 82.918212 \n",
       "L 60.874451 72.600338 \n",
       "L 61.209586 73.182965 \n",
       "L 61.544721 81.799171 \n",
       "L 61.879856 81.104509 \n",
       "L 62.214992 70.846466 \n",
       "L 62.550127 70.478822 \n",
       "L 62.885262 63.289179 \n",
       "L 63.220397 69.021074 \n",
       "L 63.555532 63.976566 \n",
       "L 63.890667 50.435822 \n",
       "L 64.225802 73.364567 \n",
       "L 64.560938 56.304511 \n",
       "L 64.896073 62.662392 \n",
       "L 65.231208 79.569811 \n",
       "L 65.566343 68.606755 \n",
       "L 65.901478 69.032175 \n",
       "L 66.236613 68.141572 \n",
       "L 66.571748 88.340765 \n",
       "L 66.906883 53.920794 \n",
       "L 67.242019 70.61777 \n",
       "L 67.577154 69.5689 \n",
       "L 67.912289 73.11776 \n",
       "L 68.247424 60.498925 \n",
       "L 68.582559 61.298357 \n",
       "L 68.917694 71.598641 \n",
       "L 69.252829 74.783299 \n",
       "L 69.587965 66.40282 \n",
       "L 69.9231 49.252079 \n",
       "L 70.258235 82.669156 \n",
       "L 70.59337 58.248636 \n",
       "L 70.928505 54.054311 \n",
       "L 71.26364 66.726904 \n",
       "L 71.598775 62.344219 \n",
       "L 71.93391 66.743895 \n",
       "L 72.269046 63.745798 \n",
       "L 72.604181 81.563764 \n",
       "L 72.939316 50.08126 \n",
       "L 73.274451 50.074763 \n",
       "L 73.609586 62.411903 \n",
       "L 73.944721 45.889762 \n",
       "L 74.279856 81.992579 \n",
       "L 74.614992 37.89197 \n",
       "L 74.950127 67.145651 \n",
       "L 75.285262 41.850544 \n",
       "L 75.620397 57.125228 \n",
       "L 75.955532 56.86214 \n",
       "L 76.290667 44.310812 \n",
       "L 76.625802 58.480374 \n",
       "L 76.960938 45.835203 \n",
       "L 77.296073 51.529777 \n",
       "L 77.631208 67.849393 \n",
       "L 77.966343 60.1513 \n",
       "L 78.301478 46.544235 \n",
       "L 78.636613 69.309296 \n",
       "L 78.971748 46.391598 \n",
       "L 79.306883 51.874733 \n",
       "L 79.642019 36.983498 \n",
       "L 80.312289 57.174192 \n",
       "L 80.647424 55.313303 \n",
       "L 80.982559 54.202752 \n",
       "L 81.317694 47.83727 \n",
       "L 81.652829 44.422589 \n",
       "L 81.987965 38.162646 \n",
       "L 82.3231 43.268602 \n",
       "L 82.99337 69.36451 \n",
       "L 83.328505 68.930909 \n",
       "L 83.66364 44.702337 \n",
       "L 83.998775 67.50203 \n",
       "L 84.33391 28.54465 \n",
       "L 84.669046 40.109348 \n",
       "L 85.339316 49.030954 \n",
       "L 85.674451 38.39113 \n",
       "L 86.009586 40.342087 \n",
       "L 86.344721 47.320184 \n",
       "L 86.679856 34.900449 \n",
       "L 87.014992 53.433026 \n",
       "L 87.685262 40.016633 \n",
       "L 88.020397 17.765776 \n",
       "L 88.355532 37.118835 \n",
       "L 88.690667 34.494395 \n",
       "L 89.025802 50.42076 \n",
       "L 89.360938 27.556847 \n",
       "L 89.696073 48.003193 \n",
       "L 90.031208 40.458374 \n",
       "L 90.366343 45.281848 \n",
       "L 90.701478 24.099 \n",
       "L 91.036613 43.993234 \n",
       "L 91.371748 26.73142 \n",
       "L 91.706883 26.825986 \n",
       "L 92.042019 40.768314 \n",
       "L 92.377154 26.007819 \n",
       "L 92.712289 43.684985 \n",
       "L 93.382559 20.729567 \n",
       "L 93.717694 45.702293 \n",
       "L 94.052829 36.430333 \n",
       "L 94.387965 46.347034 \n",
       "L 94.7231 36.02277 \n",
       "L 95.058235 43.567725 \n",
       "L 95.39337 39.494836 \n",
       "L 95.728505 50.393675 \n",
       "L 96.06364 38.33933 \n",
       "L 96.398775 43.568319 \n",
       "L 96.73391 32.738045 \n",
       "L 97.069046 57.005008 \n",
       "L 97.404181 43.618875 \n",
       "L 97.739316 25.741708 \n",
       "L 98.074451 34.065988 \n",
       "L 98.409586 31.68088 \n",
       "L 98.744721 23.716869 \n",
       "L 99.079856 18.814449 \n",
       "L 99.414992 47.796776 \n",
       "L 99.750127 48.563245 \n",
       "L 100.085262 49.8779 \n",
       "L 100.420397 31.066494 \n",
       "L 100.755532 19.542176 \n",
       "L 101.090667 54.837554 \n",
       "L 101.425802 37.365265 \n",
       "L 101.760938 44.734204 \n",
       "L 102.431208 31.75595 \n",
       "L 102.766343 23.499325 \n",
       "L 103.436613 51.441551 \n",
       "L 103.771748 49.31621 \n",
       "L 104.106883 28.809443 \n",
       "L 104.442019 16.133198 \n",
       "L 104.777154 37.025359 \n",
       "L 105.112289 31.803439 \n",
       "L 105.447424 49.774726 \n",
       "L 106.117694 14.76 \n",
       "L 106.452829 50.228044 \n",
       "L 106.787965 37.141502 \n",
       "L 107.1231 50.191171 \n",
       "L 107.458235 35.218261 \n",
       "L 107.79337 33.644712 \n",
       "L 108.128505 56.355153 \n",
       "L 108.46364 38.721963 \n",
       "L 108.798775 43.921847 \n",
       "L 109.13391 72.583997 \n",
       "L 109.804181 34.640983 \n",
       "L 110.139316 31.122863 \n",
       "L 110.474451 40.552177 \n",
       "L 110.809586 43.003627 \n",
       "L 111.144721 41.668014 \n",
       "L 111.479856 35.399794 \n",
       "L 111.814992 50.78619 \n",
       "L 112.150127 27.433863 \n",
       "L 112.485262 48.461927 \n",
       "L 112.820397 49.69198 \n",
       "L 113.155532 41.573306 \n",
       "L 113.490667 50.997469 \n",
       "L 113.825802 36.386723 \n",
       "L 114.160938 48.97352 \n",
       "L 114.496073 21.319944 \n",
       "L 114.831208 55.311588 \n",
       "L 115.166343 28.799098 \n",
       "L 115.501478 36.106117 \n",
       "L 115.836613 60.02668 \n",
       "L 116.171748 38.741742 \n",
       "L 116.506883 41.268632 \n",
       "L 116.842019 40.639846 \n",
       "L 117.177154 17.290945 \n",
       "L 117.512289 37.861845 \n",
       "L 117.847424 41.981161 \n",
       "L 118.182559 35.351505 \n",
       "L 118.517694 39.572782 \n",
       "L 118.852829 71.879805 \n",
       "L 119.187965 57.070799 \n",
       "L 119.5231 47.641362 \n",
       "L 119.858235 51.887261 \n",
       "L 120.19337 49.949507 \n",
       "L 120.86364 33.67778 \n",
       "L 121.198775 45.705658 \n",
       "L 121.53391 47.55453 \n",
       "L 121.869046 51.062908 \n",
       "L 122.204181 42.721583 \n",
       "L 122.539316 67.865456 \n",
       "L 122.874451 46.553638 \n",
       "L 123.209586 35.151412 \n",
       "L 123.544721 43.806972 \n",
       "L 123.879856 36.756206 \n",
       "L 124.214992 27.301131 \n",
       "L 124.550127 49.080266 \n",
       "L 124.885262 52.936673 \n",
       "L 125.555532 38.679886 \n",
       "L 125.890667 53.171749 \n",
       "L 126.225802 49.071137 \n",
       "L 126.560938 61.615211 \n",
       "L 126.896073 42.526979 \n",
       "L 127.566343 66.690324 \n",
       "L 127.901478 48.768671 \n",
       "L 128.236613 63.267264 \n",
       "L 128.571748 52.739416 \n",
       "L 128.906883 53.29292 \n",
       "L 129.242019 39.671446 \n",
       "L 129.577154 56.478307 \n",
       "L 129.912289 48.990534 \n",
       "L 130.247424 57.603892 \n",
       "L 130.582559 53.523447 \n",
       "L 130.917694 58.337641 \n",
       "L 131.252829 82.915523 \n",
       "L 131.587965 70.83202 \n",
       "L 131.9231 49.130662 \n",
       "L 132.258235 56.601812 \n",
       "L 132.59337 45.831521 \n",
       "L 132.928505 51.849369 \n",
       "L 133.26364 74.027545 \n",
       "L 133.598775 54.743004 \n",
       "L 133.93391 51.781069 \n",
       "L 134.269046 61.009915 \n",
       "L 134.604181 60.637107 \n",
       "L 134.939316 57.957016 \n",
       "L 135.274451 44.236204 \n",
       "L 135.944721 78.366527 \n",
       "L 136.279856 83.759056 \n",
       "L 136.614992 67.999534 \n",
       "L 136.950127 74.456658 \n",
       "L 137.285262 50.072787 \n",
       "L 137.620397 63.27913 \n",
       "L 137.955532 60.721792 \n",
       "L 138.290667 53.516338 \n",
       "L 138.625802 56.125444 \n",
       "L 138.960938 84.652247 \n",
       "L 139.296073 66.274114 \n",
       "L 139.631208 73.176695 \n",
       "L 139.966343 64.926673 \n",
       "L 140.301478 63.529572 \n",
       "L 140.636613 77.88341 \n",
       "L 141.306883 66.303425 \n",
       "L 141.642019 65.690365 \n",
       "L 141.977154 74.588622 \n",
       "L 142.312289 64.813204 \n",
       "L 142.647424 65.642952 \n",
       "L 142.982559 46.935972 \n",
       "L 143.317694 80.620216 \n",
       "L 143.652829 69.834604 \n",
       "L 144.3231 75.797436 \n",
       "L 144.658235 66.307502 \n",
       "L 144.99337 80.520627 \n",
       "L 145.328505 73.18538 \n",
       "L 145.66364 89.894464 \n",
       "L 145.998775 62.181431 \n",
       "L 146.33391 78.307772 \n",
       "L 146.669046 75.155008 \n",
       "L 147.004181 62.870764 \n",
       "L 147.339316 80.47786 \n",
       "L 147.674451 83.868137 \n",
       "L 148.009586 78.69993 \n",
       "L 148.679856 65.791657 \n",
       "L 149.014992 61.477981 \n",
       "L 149.350127 90.865868 \n",
       "L 149.685262 81.065361 \n",
       "L 150.020397 62.255325 \n",
       "L 150.355532 78.902034 \n",
       "L 150.690667 61.660084 \n",
       "L 151.025802 72.649723 \n",
       "L 151.360938 99.16062 \n",
       "L 151.696073 76.54796 \n",
       "L 152.031208 77.836355 \n",
       "L 152.366343 67.560966 \n",
       "L 152.701478 67.648759 \n",
       "L 153.036613 94.467647 \n",
       "L 153.371748 78.035113 \n",
       "L 153.706883 80.62714 \n",
       "L 154.042019 78.912643 \n",
       "L 154.377154 68.617904 \n",
       "L 154.712289 89.724973 \n",
       "L 155.047424 72.465071 \n",
       "L 155.382559 97.178381 \n",
       "L 155.717694 59.539839 \n",
       "L 156.052829 64.274153 \n",
       "L 156.387965 93.428487 \n",
       "L 156.7231 79.545538 \n",
       "L 157.058235 102.174015 \n",
       "L 157.39337 98.158537 \n",
       "L 157.728505 91.603558 \n",
       "L 158.06364 94.444845 \n",
       "L 158.398775 76.576669 \n",
       "L 158.73391 76.050357 \n",
       "L 159.069046 115.017381 \n",
       "L 159.404181 85.57195 \n",
       "L 160.074451 100.573304 \n",
       "L 160.409586 101.279253 \n",
       "L 160.744721 88.470211 \n",
       "L 161.079856 90.360925 \n",
       "L 161.414992 94.199418 \n",
       "L 161.750127 75.636569 \n",
       "L 162.085262 102.696854 \n",
       "L 162.420397 103.367424 \n",
       "L 162.755532 93.939594 \n",
       "L 163.090667 95.604194 \n",
       "L 163.425802 98.49964 \n",
       "L 163.760938 105.044589 \n",
       "L 164.096073 116.941759 \n",
       "L 164.766343 82.207519 \n",
       "L 165.101478 89.432016 \n",
       "L 165.436613 108.584569 \n",
       "L 165.771748 98.586445 \n",
       "L 166.106883 96.536858 \n",
       "L 166.442019 111.0556 \n",
       "L 166.777154 106.42555 \n",
       "L 167.112289 106.955886 \n",
       "L 167.447424 102.159287 \n",
       "L 167.782559 107.824855 \n",
       "L 168.117694 99.855127 \n",
       "L 168.787965 110.253491 \n",
       "L 169.1231 108.219525 \n",
       "L 169.458235 98.505119 \n",
       "L 169.79337 108.649371 \n",
       "L 170.128505 124.528183 \n",
       "L 170.46364 99.240288 \n",
       "L 170.798775 111.962729 \n",
       "L 171.13391 88.409507 \n",
       "L 171.469046 118.16901 \n",
       "L 171.804181 118.883547 \n",
       "L 172.474451 112.041748 \n",
       "L 172.809586 117.190699 \n",
       "L 173.144721 115.616633 \n",
       "L 173.479856 112.079249 \n",
       "L 173.814992 105.609395 \n",
       "L 174.150127 129.132336 \n",
       "L 174.485262 109.275098 \n",
       "L 174.820397 119.496094 \n",
       "L 175.155532 101.524098 \n",
       "L 175.490667 98.088649 \n",
       "L 175.825802 131.122069 \n",
       "L 176.160938 130.1583 \n",
       "L 176.496073 118.317478 \n",
       "L 176.831208 119.604925 \n",
       "L 177.166343 137.770618 \n",
       "L 177.501478 116.601394 \n",
       "L 177.836613 133.376274 \n",
       "L 178.171748 124.034054 \n",
       "L 178.506883 108.610563 \n",
       "L 178.842019 136.686961 \n",
       "L 179.177154 121.340856 \n",
       "L 179.512289 126.55808 \n",
       "L 179.847424 116.98838 \n",
       "L 180.182559 123.501515 \n",
       "L 180.517694 142.525884 \n",
       "L 180.852829 132.52443 \n",
       "L 181.187965 114.945798 \n",
       "L 181.5231 112.62213 \n",
       "L 181.858235 127.283554 \n",
       "L 182.19337 118.392121 \n",
       "L 182.528505 131.415354 \n",
       "L 182.86364 116.196082 \n",
       "L 183.198775 110.100615 \n",
       "L 183.53391 127.536271 \n",
       "L 183.869046 134.414119 \n",
       "L 184.204181 114.760581 \n",
       "L 184.539316 129.43315 \n",
       "L 184.874451 108.256489 \n",
       "L 185.879856 140.28779 \n",
       "L 186.214992 121.423599 \n",
       "L 186.550127 140.02857 \n",
       "L 186.885262 146.793259 \n",
       "L 187.220397 143.574348 \n",
       "L 187.555532 123.192225 \n",
       "L 187.890667 133.313557 \n",
       "L 188.225802 125.828869 \n",
       "L 188.560938 124.216129 \n",
       "L 188.896073 153.402625 \n",
       "L 189.231208 123.025018 \n",
       "L 189.566343 125.606433 \n",
       "L 189.901478 127.213 \n",
       "L 190.236613 129.614513 \n",
       "L 190.571748 136.049896 \n",
       "L 190.906883 148.88127 \n",
       "L 191.242019 148.724541 \n",
       "L 191.577154 163.733893 \n",
       "L 191.912289 145.075222 \n",
       "L 192.247424 147.918616 \n",
       "L 192.582559 141.680647 \n",
       "L 192.917694 130.622511 \n",
       "L 193.252829 164.544985 \n",
       "L 193.587965 141.061258 \n",
       "L 193.9231 146.906821 \n",
       "L 194.258235 132.725289 \n",
       "L 194.928505 160.124434 \n",
       "L 195.26364 122.515691 \n",
       "L 195.598775 129.546716 \n",
       "L 195.93391 142.197099 \n",
       "L 196.269046 149.441455 \n",
       "L 196.604181 143.065544 \n",
       "L 196.939316 140.615046 \n",
       "L 197.274451 131.831298 \n",
       "L 197.609586 144.918333 \n",
       "L 197.944721 145.740743 \n",
       "L 198.279856 133.344839 \n",
       "L 198.614992 132.591276 \n",
       "L 198.950127 129.889048 \n",
       "L 199.285262 145.159683 \n",
       "L 199.620397 128.431867 \n",
       "L 199.955532 135.546693 \n",
       "L 200.290667 138.482941 \n",
       "L 200.625802 142.766458 \n",
       "L 200.960938 151.948082 \n",
       "L 201.631208 141.571555 \n",
       "L 201.966343 148.267599 \n",
       "L 202.301478 137.007107 \n",
       "L 202.636613 152.272003 \n",
       "L 202.971748 138.324521 \n",
       "L 203.306883 129.132841 \n",
       "L 203.642019 143.190216 \n",
       "L 203.977154 136.596355 \n",
       "L 204.312289 126.485815 \n",
       "L 204.647424 155.29648 \n",
       "L 204.982559 149.234299 \n",
       "L 205.317694 154.601396 \n",
       "L 205.652829 151.723321 \n",
       "L 206.3231 126.406813 \n",
       "L 206.658235 137.508928 \n",
       "L 206.99337 122.332022 \n",
       "L 207.328505 141.752404 \n",
       "L 207.66364 145.515335 \n",
       "L 207.998775 138.158971 \n",
       "L 208.33391 134.539444 \n",
       "L 208.669046 154.894472 \n",
       "L 209.004181 139.873934 \n",
       "L 209.339316 138.680995 \n",
       "L 209.674451 130.941974 \n",
       "L 210.009586 137.158689 \n",
       "L 210.344721 141.068259 \n",
       "L 210.679856 138.977055 \n",
       "L 211.014992 146.83116 \n",
       "L 211.350127 133.385554 \n",
       "L 211.685262 135.721841 \n",
       "L 212.020397 129.000777 \n",
       "L 212.355532 130.85267 \n",
       "L 212.690667 130.731446 \n",
       "L 213.025802 124.784903 \n",
       "L 213.360938 145.17993 \n",
       "L 213.696073 131.959941 \n",
       "L 214.031208 128.536538 \n",
       "L 214.366343 147.547612 \n",
       "L 214.701478 125.555027 \n",
       "L 215.036613 136.265975 \n",
       "L 215.371748 142.527454 \n",
       "L 215.706883 130.836604 \n",
       "L 216.042019 165.96 \n",
       "L 216.377154 160.657862 \n",
       "L 216.712289 144.619542 \n",
       "L 217.047424 145.610529 \n",
       "L 217.382559 142.848364 \n",
       "L 217.717694 144.62836 \n",
       "L 218.052829 149.699487 \n",
       "L 218.387965 140.424556 \n",
       "L 218.7231 137.461414 \n",
       "L 219.058235 143.18266 \n",
       "L 219.39337 122.041002 \n",
       "L 219.728505 122.066668 \n",
       "L 220.06364 134.082889 \n",
       "L 220.398775 129.914338 \n",
       "L 221.069046 151.275739 \n",
       "L 221.404181 145.218195 \n",
       "L 221.739316 141.694656 \n",
       "L 222.074451 136.952903 \n",
       "L 222.409586 134.688633 \n",
       "L 222.744721 148.987382 \n",
       "L 223.079856 122.120284 \n",
       "L 223.414992 123.2747 \n",
       "L 223.750127 122.95501 \n",
       "L 224.085262 140.480093 \n",
       "L 224.420397 120.224237 \n",
       "L 224.755532 118.900573 \n",
       "L 225.090667 129.822651 \n",
       "L 225.425802 147.777817 \n",
       "L 225.760938 125.765638 \n",
       "L 226.096073 119.877637 \n",
       "L 226.431208 147.09027 \n",
       "L 227.101478 122.13566 \n",
       "L 227.436613 140.938187 \n",
       "L 227.771748 123.567856 \n",
       "L 228.106883 147.551461 \n",
       "L 228.442019 147.663829 \n",
       "L 228.777154 128.286194 \n",
       "L 229.112289 128.934593 \n",
       "L 229.447424 126.542729 \n",
       "L 229.782559 121.23181 \n",
       "L 230.117694 143.946472 \n",
       "L 230.452829 132.772991 \n",
       "L 230.787965 132.924378 \n",
       "L 231.1231 133.770397 \n",
       "L 231.458235 155.877595 \n",
       "L 231.79337 133.91462 \n",
       "L 232.128505 145.491991 \n",
       "L 232.46364 120.240457 \n",
       "L 232.798775 133.318896 \n",
       "L 233.13391 131.049369 \n",
       "L 233.469046 118.522796 \n",
       "L 233.804181 126.422226 \n",
       "L 234.139316 93.932235 \n",
       "L 234.474451 118.563647 \n",
       "L 234.809586 109.620185 \n",
       "L 235.144721 132.374859 \n",
       "L 235.479856 114.685648 \n",
       "L 235.814992 125.714013 \n",
       "L 236.150127 108.67343 \n",
       "L 236.485262 108.227984 \n",
       "L 236.820397 118.256131 \n",
       "L 237.155532 136.429471 \n",
       "L 237.490667 128.989866 \n",
       "L 237.825802 127.082947 \n",
       "L 238.160938 137.467301 \n",
       "L 238.496073 126.330557 \n",
       "L 238.831208 145.450715 \n",
       "L 239.166343 130.603425 \n",
       "L 239.501478 127.96776 \n",
       "L 239.836613 123.585963 \n",
       "L 240.171748 123.999465 \n",
       "L 240.506883 127.501667 \n",
       "L 240.842019 115.23589 \n",
       "L 241.177154 120.790607 \n",
       "L 241.512289 106.591927 \n",
       "L 241.847424 126.263182 \n",
       "L 242.182559 106.844768 \n",
       "L 242.517694 121.303207 \n",
       "L 242.852829 122.817391 \n",
       "L 243.187965 114.558952 \n",
       "L 243.5231 112.257622 \n",
       "L 243.858235 113.818329 \n",
       "L 244.19337 112.074496 \n",
       "L 244.528505 131.700912 \n",
       "L 244.86364 116.418229 \n",
       "L 245.198775 119.855296 \n",
       "L 245.53391 124.471685 \n",
       "L 245.869046 121.351343 \n",
       "L 246.204181 109.945236 \n",
       "L 246.539316 117.400903 \n",
       "L 246.874451 116.070037 \n",
       "L 247.209586 113.725379 \n",
       "L 247.544721 112.208713 \n",
       "L 247.879856 107.400659 \n",
       "L 248.214992 110.85309 \n",
       "L 248.550127 125.157502 \n",
       "L 248.885262 117.73438 \n",
       "L 249.220397 120.275704 \n",
       "L 249.555532 119.471546 \n",
       "L 249.890667 113.491748 \n",
       "L 250.225802 112.709338 \n",
       "L 250.560938 121.775248 \n",
       "L 250.896073 126.700089 \n",
       "L 251.231208 128.633237 \n",
       "L 251.566343 111.909058 \n",
       "L 251.901478 129.566421 \n",
       "L 252.236613 120.745695 \n",
       "L 252.571748 96.520033 \n",
       "L 252.906883 87.036743 \n",
       "L 253.242019 95.046692 \n",
       "L 253.577154 89.672807 \n",
       "L 253.912289 107.837551 \n",
       "L 254.247424 90.686768 \n",
       "L 254.582559 110.665098 \n",
       "L 254.917694 112.313387 \n",
       "L 255.252829 84.827498 \n",
       "L 255.587965 86.309725 \n",
       "L 255.9231 90.086487 \n",
       "L 256.59337 112.179177 \n",
       "L 256.928505 108.088394 \n",
       "L 257.26364 97.588989 \n",
       "L 257.598775 101.147874 \n",
       "L 257.93391 95.46569 \n",
       "L 258.269046 98.021317 \n",
       "L 258.604181 111.690502 \n",
       "L 258.939316 89.591656 \n",
       "L 259.274451 97.984283 \n",
       "L 259.609586 92.315527 \n",
       "L 259.944721 94.414804 \n",
       "L 260.950127 81.655089 \n",
       "L 261.285262 92.710732 \n",
       "L 261.620397 110.920097 \n",
       "L 261.955532 81.521274 \n",
       "L 262.290667 94.286859 \n",
       "L 262.625802 97.429167 \n",
       "L 262.960938 106.645583 \n",
       "L 263.631208 84.219073 \n",
       "L 263.966343 99.879376 \n",
       "L 264.301478 80.243224 \n",
       "L 264.636613 75.783475 \n",
       "L 265.306883 102.691085 \n",
       "L 265.642019 83.996391 \n",
       "L 265.977154 96.903655 \n",
       "L 266.312289 76.862587 \n",
       "L 266.647424 87.827038 \n",
       "L 266.982559 90.187995 \n",
       "L 267.317694 73.734404 \n",
       "L 267.652829 74.523288 \n",
       "L 267.987965 74.706169 \n",
       "L 268.3231 71.274716 \n",
       "L 268.658235 95.016828 \n",
       "L 268.99337 73.76408 \n",
       "L 269.328505 73.189066 \n",
       "L 269.66364 77.336891 \n",
       "L 269.998775 86.684367 \n",
       "L 270.33391 72.451483 \n",
       "L 270.669046 98.014393 \n",
       "L 271.004181 89.385096 \n",
       "L 271.339316 75.536483 \n",
       "L 271.674451 80.977456 \n",
       "L 272.009586 74.911585 \n",
       "L 272.344721 95.227796 \n",
       "L 272.679856 79.107849 \n",
       "L 273.014992 86.579073 \n",
       "L 273.350127 59.351018 \n",
       "L 273.685262 88.737612 \n",
       "L 274.020397 64.980552 \n",
       "L 274.355532 85.582492 \n",
       "L 274.690667 59.867737 \n",
       "L 275.025802 55.360915 \n",
       "L 275.696073 68.465651 \n",
       "L 276.031208 77.608823 \n",
       "L 276.366343 71.847674 \n",
       "L 276.701478 75.467623 \n",
       "L 277.371748 86.790148 \n",
       "L 277.706883 62.111696 \n",
       "L 278.042019 50.249049 \n",
       "L 278.377154 61.3863 \n",
       "L 278.712289 57.913058 \n",
       "L 279.047424 62.690912 \n",
       "L 279.382559 75.094022 \n",
       "L 279.717694 51.922176 \n",
       "L 280.052829 71.604812 \n",
       "L 280.387965 68.733552 \n",
       "L 280.7231 59.929002 \n",
       "L 281.058235 72.484151 \n",
       "L 281.39337 53.00303 \n",
       "L 281.728505 41.234043 \n",
       "L 282.398775 58.521139 \n",
       "L 282.73391 63.381599 \n",
       "L 283.069046 58.591541 \n",
       "L 283.404181 63.933986 \n",
       "L 283.739316 53.471026 \n",
       "L 284.074451 70.30042 \n",
       "L 284.409586 46.37999 \n",
       "L 284.744721 53.869724 \n",
       "L 285.079856 35.368396 \n",
       "L 285.414992 43.121118 \n",
       "L 285.750127 71.010723 \n",
       "L 286.085262 45.947067 \n",
       "L 286.420397 50.514414 \n",
       "L 286.755532 32.415762 \n",
       "L 287.090667 58.787062 \n",
       "L 287.425802 70.90354 \n",
       "L 287.760938 59.928087 \n",
       "L 288.096073 66.955771 \n",
       "L 288.431208 55.285494 \n",
       "L 288.766343 50.481969 \n",
       "L 289.101478 37.847294 \n",
       "L 289.771748 69.51024 \n",
       "L 290.106883 55.74372 \n",
       "L 290.442019 57.994794 \n",
       "L 290.777154 39.192554 \n",
       "L 291.112289 42.822122 \n",
       "L 291.447424 42.269727 \n",
       "L 291.782559 46.408276 \n",
       "L 292.117694 39.565928 \n",
       "L 292.452829 45.736265 \n",
       "L 292.787965 40.370253 \n",
       "L 293.1231 50.032754 \n",
       "L 293.458235 43.07297 \n",
       "L 293.79337 45.289265 \n",
       "L 294.128505 55.643341 \n",
       "L 294.46364 62.312393 \n",
       "L 294.798775 44.682678 \n",
       "L 295.13391 45.426372 \n",
       "L 295.469046 58.122754 \n",
       "L 295.804181 53.056634 \n",
       "L 296.139316 58.111338 \n",
       "L 296.474451 56.57323 \n",
       "L 296.809586 49.667382 \n",
       "L 297.144721 52.961449 \n",
       "L 297.479856 65.510811 \n",
       "L 297.814992 38.034991 \n",
       "L 298.150127 55.824828 \n",
       "L 298.485262 41.188675 \n",
       "L 298.820397 33.189082 \n",
       "L 299.155532 45.990874 \n",
       "L 299.490667 46.549189 \n",
       "L 299.825802 46.632502 \n",
       "L 300.160938 42.597453 \n",
       "L 300.496073 45.747153 \n",
       "L 300.831208 27.896341 \n",
       "L 301.166343 31.376258 \n",
       "L 301.501478 36.175091 \n",
       "L 301.836613 34.164539 \n",
       "L 302.171748 50.454619 \n",
       "L 302.506883 37.351946 \n",
       "L 302.842019 40.087943 \n",
       "L 303.177154 54.558561 \n",
       "L 303.512289 19.853504 \n",
       "L 303.847424 38.049616 \n",
       "L 304.182559 37.853772 \n",
       "L 304.517694 41.748127 \n",
       "L 304.852829 52.667902 \n",
       "L 305.187965 35.681786 \n",
       "L 305.5231 51.013107 \n",
       "L 305.858235 29.670094 \n",
       "L 306.19337 22.159663 \n",
       "L 306.528505 46.313593 \n",
       "L 306.86364 33.241035 \n",
       "L 307.53391 37.256838 \n",
       "L 307.869046 27.258894 \n",
       "L 308.204181 46.40343 \n",
       "L 308.539316 48.68062 \n",
       "L 308.874451 43.285068 \n",
       "L 309.209586 35.321362 \n",
       "L 309.544721 33.428172 \n",
       "L 309.879856 43.767479 \n",
       "L 310.214992 49.474152 \n",
       "L 310.550127 46.099442 \n",
       "L 310.885262 47.412503 \n",
       "L 311.220397 42.331958 \n",
       "L 311.555532 43.849615 \n",
       "L 311.890667 44.752089 \n",
       "L 312.225802 38.409582 \n",
       "L 312.560938 38.885229 \n",
       "L 312.896073 38.813419 \n",
       "L 313.231208 37.022144 \n",
       "L 313.566343 46.496515 \n",
       "L 313.901478 45.131067 \n",
       "L 314.236613 30.701648 \n",
       "L 314.571748 55.228251 \n",
       "L 314.906883 34.427213 \n",
       "L 315.242019 43.247665 \n",
       "L 315.577154 35.595724 \n",
       "L 315.912289 40.135639 \n",
       "L 316.247424 42.803023 \n",
       "L 316.582559 34.568511 \n",
       "L 316.917694 50.646974 \n",
       "L 317.252829 45.514854 \n",
       "L 317.587965 26.052919 \n",
       "L 317.9231 36.493174 \n",
       "L 318.258235 33.294036 \n",
       "L 318.59337 44.203133 \n",
       "L 318.928505 26.459625 \n",
       "L 319.598775 43.645434 \n",
       "L 320.269046 25.84801 \n",
       "L 320.604181 40.355172 \n",
       "L 320.939316 46.848743 \n",
       "L 321.274451 31.673829 \n",
       "L 321.609586 22.516719 \n",
       "L 321.944721 33.698082 \n",
       "L 322.614992 34.592945 \n",
       "L 322.950127 50.497745 \n",
       "L 323.285262 42.88838 \n",
       "L 323.620397 28.923234 \n",
       "L 323.955532 48.265289 \n",
       "L 324.290667 41.703135 \n",
       "L 324.625802 56.707656 \n",
       "L 324.960938 30.537782 \n",
       "L 325.296073 38.076203 \n",
       "L 325.631208 50.781671 \n",
       "L 325.966343 47.343319 \n",
       "L 326.301478 23.961976 \n",
       "L 326.636613 37.274326 \n",
       "L 326.971748 56.471019 \n",
       "L 327.306883 28.129446 \n",
       "L 327.642019 42.5651 \n",
       "L 327.977154 48.878871 \n",
       "L 328.312289 58.713396 \n",
       "L 328.647424 31.185001 \n",
       "L 328.982559 33.788646 \n",
       "L 329.317694 40.151637 \n",
       "L 329.652829 30.578953 \n",
       "L 329.987965 33.306462 \n",
       "L 330.3231 59.837533 \n",
       "L 330.658235 50.107163 \n",
       "L 330.99337 47.866375 \n",
       "L 331.328505 46.734266 \n",
       "L 331.66364 39.419344 \n",
       "L 331.998775 50.373646 \n",
       "L 332.33391 39.015596 \n",
       "L 332.669046 48.474705 \n",
       "L 333.004181 44.240742 \n",
       "L 333.339316 35.300499 \n",
       "L 333.674451 60.251617 \n",
       "L 334.009586 47.737313 \n",
       "L 334.344721 46.098479 \n",
       "L 334.679856 59.60917 \n",
       "L 335.014992 59.801956 \n",
       "L 335.350127 35.208612 \n",
       "L 335.685262 48.788059 \n",
       "L 336.020397 49.299039 \n",
       "L 336.355532 49.308451 \n",
       "L 336.690667 51.927053 \n",
       "L 337.025802 47.388025 \n",
       "L 337.360937 59.573331 \n",
       "L 337.696073 46.345337 \n",
       "L 338.031208 49.158674 \n",
       "L 338.366343 38.729257 \n",
       "L 339.036613 60.612697 \n",
       "L 339.371748 50.797256 \n",
       "L 339.706883 55.83703 \n",
       "L 340.042019 42.578788 \n",
       "L 340.377154 49.422833 \n",
       "L 341.047424 57.734767 \n",
       "L 341.382559 40.610356 \n",
       "L 341.717694 64.051703 \n",
       "L 342.052829 45.354985 \n",
       "L 342.387965 52.374638 \n",
       "L 342.7231 46.02032 \n",
       "L 343.058235 46.972808 \n",
       "L 343.39337 51.757118 \n",
       "L 343.728505 60.576347 \n",
       "L 344.06364 49.135062 \n",
       "L 344.398775 54.666166 \n",
       "L 344.73391 75.747756 \n",
       "L 345.404181 47.021052 \n",
       "L 345.739316 56.679705 \n",
       "L 346.074451 59.173789 \n",
       "L 346.409586 70.383509 \n",
       "L 346.744721 46.275107 \n",
       "L 347.079856 66.790448 \n",
       "L 347.414992 57.814105 \n",
       "L 347.750127 68.61178 \n",
       "L 348.085262 65.919344 \n",
       "L 348.420397 51.223893 \n",
       "L 348.755532 51.218508 \n",
       "L 349.090667 53.370145 \n",
       "L 349.425802 65.511754 \n",
       "L 349.760938 69.210875 \n",
       "L 350.096073 43.805525 \n",
       "L 350.766343 73.397649 \n",
       "L 351.101478 64.311497 \n",
       "L 351.436613 65.036213 \n",
       "L 351.771748 72.000607 \n",
       "L 352.106883 68.858161 \n",
       "L 352.777154 49.386964 \n",
       "L 353.112289 72.294733 \n",
       "L 353.447424 46.187666 \n",
       "L 353.782559 71.879737 \n",
       "L 354.117694 55.641685 \n",
       "L 354.452829 76.504156 \n",
       "L 354.787965 79.351888 \n",
       "L 355.1231 65.609217 \n",
       "L 355.458235 59.016635 \n",
       "L 356.128505 84.981329 \n",
       "L 356.46364 68.041778 \n",
       "L 356.798775 66.524221 \n",
       "L 357.13391 72.287959 \n",
       "L 357.469046 86.431849 \n",
       "L 357.804181 67.737444 \n",
       "L 358.139316 82.446348 \n",
       "L 358.474451 74.915593 \n",
       "L 358.809586 90.531737 \n",
       "L 359.144721 65.349211 \n",
       "L 359.479856 96.865746 \n",
       "L 359.814992 66.487003 \n",
       "L 360.150127 75.952595 \n",
       "L 360.485262 55.4164 \n",
       "L 360.820397 78.719687 \n",
       "L 361.155532 81.348646 \n",
       "L 361.490667 68.726386 \n",
       "L 361.825802 62.348185 \n",
       "L 362.160938 100.877244 \n",
       "L 362.496073 65.086189 \n",
       "L 362.831208 95.308649 \n",
       "L 363.166343 102.005875 \n",
       "L 363.501478 83.551298 \n",
       "L 363.836613 79.662035 \n",
       "L 364.171748 70.486738 \n",
       "L 364.506883 78.923451 \n",
       "L 364.842019 83.022334 \n",
       "L 365.177154 76.341838 \n",
       "L 365.512289 76.423368 \n",
       "L 365.847424 82.684427 \n",
       "L 366.182559 101.796082 \n",
       "L 366.517694 86.64841 \n",
       "L 367.187965 78.285248 \n",
       "L 367.5231 84.369141 \n",
       "L 367.858235 75.367601 \n",
       "L 368.19337 106.44165 \n",
       "L 368.528505 75.544015 \n",
       "L 368.86364 87.371883 \n",
       "L 369.198775 89.423534 \n",
       "L 369.53391 79.770896 \n",
       "L 370.204181 97.042015 \n",
       "L 370.539316 75.258053 \n",
       "L 370.874451 98.517165 \n",
       "L 371.209586 85.609532 \n",
       "L 371.544721 91.11424 \n",
       "L 371.879856 120.274497 \n",
       "L 372.214992 88.871663 \n",
       "L 372.550127 81.554069 \n",
       "L 372.885262 86.763218 \n",
       "L 373.220397 94.677021 \n",
       "L 373.555532 105.612809 \n",
       "L 373.890667 86.178807 \n",
       "L 374.225802 104.112971 \n",
       "L 374.560938 110.705056 \n",
       "L 374.896073 81.232893 \n",
       "L 375.231208 85.173102 \n",
       "L 375.566343 106.095878 \n",
       "L 375.901478 101.42294 \n",
       "L 376.236613 99.028054 \n",
       "L 376.571748 107.211539 \n",
       "L 376.906883 98.429624 \n",
       "L 377.242019 113.958424 \n",
       "L 377.577154 92.355284 \n",
       "L 377.912289 94.425388 \n",
       "L 378.582559 109.260881 \n",
       "L 378.917694 108.183285 \n",
       "L 379.252829 114.572294 \n",
       "L 379.587965 114.356833 \n",
       "L 379.9231 103.753549 \n",
       "L 380.258235 117.64003 \n",
       "L 380.59337 101.201017 \n",
       "L 381.26364 127.481845 \n",
       "L 381.598775 104.980703 \n",
       "L 381.93391 111.082252 \n",
       "L 382.604181 93.927304 \n",
       "L 382.939316 108.64932 \n",
       "L 383.274451 111.03764 \n",
       "L 383.609586 131.879375 \n",
       "L 383.944721 133.877935 \n",
       "L 384.279856 98.161056 \n",
       "L 384.614992 120.096573 \n",
       "L 384.950127 125.397873 \n",
       "L 385.620397 110.486944 \n",
       "L 385.955532 130.024268 \n",
       "L 386.290667 125.744673 \n",
       "L 386.625802 133.424786 \n",
       "L 386.960938 122.085606 \n",
       "L 386.960938 122.085606 \n",
       "\" clip-path=\"url(#pc1d0006aff)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 173.52 \n",
       "L 52.160938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 386.960938 173.52 \n",
       "L 386.960938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 173.52 \n",
       "L 386.960938 173.52 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 7.2 \n",
       "L 386.960938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pc1d0006aff\">\n",
       "   <rect x=\"52.160938\" y=\"7.2\" width=\"334.8\" height=\"166.32\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import d2l\n",
    "\n",
    "T = 1000  # 总共产生1000个点\n",
    "time = torch.arange(1, T + 1, dtype=torch.float32)\n",
    "x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))\n",
    "d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200180a0-63c6-4f83-a9c1-f6c42895ba4f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "接下来，我们将这个序列转换为模型的*特征－标签*（feature-label）对。\n",
    "基于嵌入维度$\\tau$，我们[**将数据映射为数据对$y_t = x_t$\n",
    "和$\\mathbf{x}_t = [x_{t-\\tau}, \\ldots, x_{t-1}]$。**]\n",
    "这比我们提供的数据样本少了$\\tau$个，\n",
    "因为我们没有足够的历史记录来描述前$\\tau$个数据样本。\n",
    "一个简单的解决办法是：如果拥有足够长的序列就丢弃这几项；\n",
    "另一个方法是用零填充序列。\n",
    "在这里，我们仅使用前600个“特征－标签”对进行训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49480f9e-e7f3-4fa4-b849-62e11ff3128d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tau = 4\n",
    "features = torch.zeros((T - tau, tau))\n",
    "for i in range(tau):\n",
    "    features[:, i] = x[i: T - tau + i]\n",
    "labels = x[tau:].reshape((-1, 1))\n",
    "\n",
    "batch_size, n_train = 16, 600\n",
    "# 只有前n_train个样本用于训练\n",
    "train_iter = d2l.load_array((features[:n_train], labels[:n_train]), batch_size, is_train=True)\n",
    "\n",
    "# 使用一个相当简单的架构训练模型：一个拥有两个全连接层的多层感知机，ReLU激活函数和平方损失。\n",
    "# 初始化网络权重的函数\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# 一个简单的多层感知机\n",
    "def get_net():\n",
    "    net = nn.Sequential(nn.Linear(4, 10),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(10, 1))\n",
    "    net.apply(init_weights)\n",
    "    return net\n",
    "\n",
    "# 平方损失。注意：MSELoss计算平方误差时不带系数1/2\n",
    "loss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5de892-718f-4ad4-b563-02f30f9ac777",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2.2 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a0227-a6c7-4e14-a72e-2e9a908b5096",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, loss, epochs, lr):\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            l = loss(net(X), y)\n",
    "            l.sum().backward()\n",
    "            trainer.step()\n",
    "        print(f'epoch {epoch + 1}, '\n",
    "              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')\n",
    "\n",
    "net = get_net()\n",
    "train(net, train_iter, loss, 5, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba19ccc-5f50-481d-8aeb-5e146af94009",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2.3 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a7d8e-61ee-4719-9516-4fa62a2ff079",
   "metadata": {},
   "source": [
    "由于训练损失很小，因此我们期望模型能有很好的工作效果。\n",
    "让我们看看这在实践中意味着什么。\n",
    "首先是检查[**模型预测下一个时间步**]的能力，\n",
    "也就是*单步预测*（one-step-ahead prediction）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8040b04-c5b6-49c5-96ad-f923ba2ccbaf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "onestep_preds = net(features)\n",
    "d2l.plot([time, time[tau:]],\n",
    "         [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',\n",
    "         'x', legend=['data', '1-step preds'], xlim=[1, 1000],\n",
    "         figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529abdf7-5bc3-48cc-aa76-36d8138f067b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "正如我们所料，单步预测效果不错。\n",
    "即使这些预测的时间步超过了$600+4$（`n_train + tau`），\n",
    "其结果看起来仍然是可信的。\n",
    "然而有一个小问题：如果数据观察序列的时间步只到$604$，\n",
    "我们需要一步一步地向前迈进：\n",
    "$$\n",
    "\\hat{x}_{605} = f(x_{601}, x_{602}, x_{603}, x_{604}), \\\\\n",
    "\\hat{x}_{606} = f(x_{602}, x_{603}, x_{604}, \\hat{x}_{605}), \\\\\n",
    "\\hat{x}_{607} = f(x_{603}, x_{604}, \\hat{x}_{605}, \\hat{x}_{606}),\\\\\n",
    "\\hat{x}_{608} = f(x_{604}, \\hat{x}_{605}, \\hat{x}_{606}, \\hat{x}_{607}),\\\\\n",
    "\\hat{x}_{609} = f(\\hat{x}_{605}, \\hat{x}_{606}, \\hat{x}_{607}, \\hat{x}_{608}),\\\\\n",
    "\\ldots\n",
    "$$\n",
    "\n",
    "通常，对于直到$x_t$的观测序列，其在时间步$t+k$处的预测输出$\\hat{x}_{t+k}$\n",
    "称为$k$*步预测*（$k$-step-ahead-prediction）。\n",
    "由于我们的观察已经到了$x_{604}$，它的$k$步预测是$\\hat{x}_{604+k}$。\n",
    "换句话说，我们必须使用我们自己的预测（而不是原始数据）来[**进行多步预测**]。\n",
    "让我们看看效果如何。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0c19d-87fd-4d6a-8fe3-78d7fda1342c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "multistep_preds = torch.zeros(T)\n",
    "multistep_preds[: n_train + tau] = x[: n_train + tau]\n",
    "for i in range(n_train + tau, T):\n",
    "    multistep_preds[i] = net(\n",
    "        multistep_preds[i - tau:i].reshape((1, -1)))\n",
    "\n",
    "d2l.plot([time, time[tau:], time[n_train + tau:]],\n",
    "         [x.detach().numpy(), onestep_preds.detach().numpy(),\n",
    "          multistep_preds[n_train + tau:].detach().numpy()], 'time',\n",
    "         'x', legend=['data', '1-step preds', 'multistep preds'],\n",
    "         xlim=[1, 1000], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c83e27-c4dc-4f3f-8b23-d0434a8158fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "如上面的例子所示，绿线的预测显然并不理想。\n",
    "经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。\n",
    "为什么这个算法效果这么差呢？事实是由于错误的累积：\n",
    "假设在步骤$1$之后，我们积累了一些错误$\\epsilon_1 = \\bar\\epsilon$。\n",
    "于是，步骤$2$的输入被扰动了$\\epsilon_1$，\n",
    "结果积累的误差是依照次序的$\\epsilon_2 = \\bar\\epsilon + c \\epsilon_1$，\n",
    "其中$c$为某个常数，后面的预测误差依此类推。\n",
    "因此误差可能会相当快地偏离真实的观测结果。\n",
    "例如，未来$24$小时的天气预报往往相当准确，\n",
    "但超过这一点，精度就会迅速下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64260684-1b1a-4f49-8697-fd78e88f7d44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 3. 文本预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d8bfe-ddcd-4668-8d49-ca03bb8f9f02",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3.1 读取数据集\n",
    "\n",
    "首先，我们从H.G.Well的[时光机器](https://www.gutenberg.org/ebooks/35)中加载文本。\n",
    "这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀，\n",
    "而现实中的文档集合可能会包含数十亿个单词。\n",
    "下面的函数(**将数据集读取到由多条文本行组成的列表中**)，其中每条文本行都是一个字符串。\n",
    "为简单起见，我们在这里忽略了标点符号和字母大写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64bd75-d400-4cde-b579-5097be696855",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_time_machine():  #@save\n",
    "    \"\"\"将时间机器数据集加载到文本行的列表中\"\"\"\n",
    "    with open('../data/timemachine.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()\n",
    "print(f'# 文本总行数: {len(lines)}')\n",
    "print(lines[0])\n",
    "print(lines[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2bec61-3162-42b4-b931-270ffa775df3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bfd410-f1ab-47ff-acfb-627793679915",
   "metadata": {},
   "source": [
    "## 3.2 词元化（tokenize）\n",
    "\n",
    "下面的`tokenize`函数将文本行列表（`lines`）作为输入，\n",
    "列表中的每个元素是一个文本序列（如一条文本行）。\n",
    "[**每个文本序列又被拆分成一个词元列表**]，*词元*（token）是文本的基本单位。\n",
    "最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525fc48-6915-4e9d-8bd6-6336eb2ea4b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)\n",
    "\n",
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7567f3-fffd-4b83-8bf3-c9522e832d1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3.3 词表（Vocab）\n",
    "\n",
    "词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。\n",
    "现在，让我们[**构建一个字典，通常也叫做*词表*（vocabulary），\n",
    "用来将字符串类型的词元映射到从$0$开始的数字索引中**]。\n",
    "我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，\n",
    "得到的统计结果称之为*语料*（corpus）。\n",
    "然后根据每个唯一词元的出现频率，为其分配一个数字索引。\n",
    "很少出现的词元通常被移除，这可以降低复杂性。\n",
    "另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“&lt;unk&gt;”。\n",
    "我们可以选择增加一个列表，用于保存那些被保留的词元，\n",
    "例如：填充词元（“&lt;pad&gt;”）；\n",
    "序列开始词元（“&lt;bos&gt;”）；\n",
    "序列结束词元（“&lt;eos&gt;”）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15bd526-27f8-4601-a787-05b0e225b62b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oov out of vocab\n",
    "\n",
    "class Vocab:  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e0401-235a-45cc-8101-f507a2982958",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 现在，我们可以将每一条文本行转换成一个数字索引列表\n",
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10])\n",
    "\n",
    "for i in [0, 10]:\n",
    "    print('文本:', tokens[i])\n",
    "    print('索引:', vocab[tokens[i]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988c808-3cbd-4c16-8092-7a6b27583683",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3.4 整合所有功能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42332d9-63d3-4b17-a8ff-84d022979813",
   "metadata": {},
   "source": [
    "在使用上述函数时，我们[**将所有功能打包到`load_corpus_time_machine`函数中**]，\n",
    "该函数返回`corpus`（词元索引列表）和`vocab`（时光机器语料库的词表）。\n",
    "我们在这里所做的改变是：\n",
    "\n",
    "1. 为了简化后面章节中的训练，我们使用字符（而不是单词）实现文本词元化；\n",
    "1. 时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的`corpus`仅处理为单个列表，而不是使用多词元列表构成的一个列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc97cb7-d27a-4aa0-89e6-3a3a63743cb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8ab1d-0f42-45ef-a422-007e7161855a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 4. 语言模型和数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86aed2f-872e-43a2-a465-bf6403b90e3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "假设长度为$T$的文本序列中的词元依次为$x_1, x_2, \\ldots, x_T$。\n",
    "于是，$x_t$（$1 \\leq t \\leq T$）\n",
    "可以被认为是文本序列在时间步$t$处的观测或标签。\n",
    "在给定这样的文本序列时，*语言模型*（language model）的目标是估计序列的联合概率\n",
    "\n",
    "$$P(x_1, x_2, \\ldots, x_T).$$\n",
    "\n",
    "例如，只需要一次抽取一个词元$x_t \\sim P(x_t \\mid x_{t-1}, \\ldots, x_1)$，\n",
    "一个理想的语言模型就能够基于模型本身生成自然文本。\n",
    "与猴子使用打字机完全不同的是，从这样的模型中提取的文本\n",
    "都将作为自然语言（例如，英语文本）来传递。\n",
    "只需要基于前面的对话片断中的文本，\n",
    "就足以生成一个有意义的对话。\n",
    "显然，我们离设计出这样的系统还很遥远，\n",
    "因为它需要“理解”文本，而不仅仅是生成语法合理的内容。\n",
    "\n",
    "尽管如此，语言模型依然是非常有用的。\n",
    "例如，短语“to recognize speech”和“to wreck a nice beach”读音上听起来非常相似。\n",
    "这种相似性会导致语音识别中的歧义，但是这很容易通过语言模型来解决，\n",
    "因为第二句的语义很奇怪。\n",
    "同样，在文档摘要生成算法中，\n",
    "“狗咬人”比“人咬狗”出现的频率要高得多，\n",
    "或者“我想吃奶奶”是一个相当匪夷所思的语句，\n",
    "而“我想吃，奶奶”则要正常得多。\n",
    "\n",
    "**学习语言模型**\n",
    "\n",
    "显而易见，我们面对的问题是如何对一个文档，\n",
    "甚至是一个词元序列进行建模。\n",
    "假设在单词级别对文本数据进行词元化，从基本概率规则开始：\n",
    "\n",
    "$$P(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^T P(x_t  \\mid  x_1, \\ldots, x_{t-1}).$$\n",
    "\n",
    "例如，包含了四个单词的一个文本序列的概率是：\n",
    "\n",
    "$$P(\\text{deep}, \\text{learning}, \\text{is}, \\text{fun}) =  P(\\text{deep}) P(\\text{learning}  \\mid  \\text{deep}) P(\\text{is}  \\mid  \\text{deep}, \\text{learning}) P(\\text{fun}  \\mid  \\text{deep}, \\text{learning}, \\text{is}).$$\n",
    "\n",
    "为了训练语言模型，我们需要计算单词的概率，\n",
    "以及给定前面几个单词后出现某个单词的条件概率。\n",
    "这些概率本质上就是语言模型的参数。\n",
    "\n",
    "这里，我们假设训练数据集是一个大型的文本语料库。\n",
    "比如，维基百科的所有条目、古登堡计划或者所有发布在网络上的文本。\n",
    "训练数据集中词的概率可以根据给定词的相对词频来计算。\n",
    "例如，可以将估计值$\\hat{P}(\\text{deep})$\n",
    "计算为任何以单词“deep”开头的句子的概率。\n",
    "一种（稍稍不太精确的）方法是统计单词“deep”在数据集中的出现次数，\n",
    "然后将其除以整个语料库中的单词总数。\n",
    "这种方法效果不错，特别是对于频繁出现的单词。\n",
    "接下来，我们可以尝试估计\n",
    "\n",
    "$$\\hat{P}(\\text{learning} \\mid \\text{deep}) = \\frac{n(\\text{deep, learning})}{n(\\text{deep})},$$\n",
    "\n",
    "其中$n(x)$和$n(x, x')$分别是单个单词和连续单词对的出现次数。\n",
    "不幸的是，由于连续单词对“deep learning”的出现频率要低得多，\n",
    "所以估计这类单词正确的概率要困难得多。\n",
    "特别是对于一些不常见的单词组合，要想找到足够的出现次数来获得准确的估计可能都不容易。\n",
    "而对于三个或者更多的单词组合，情况会变得更糟。\n",
    "许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。\n",
    "除非我们提供某种解决方案，来将这些单词组合指定为非零计数，\n",
    "否则将无法在语言模型中使用它们。\n",
    "如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。\n",
    "\n",
    "一种常见的策略是执行某种形式的*拉普拉斯平滑*（Laplace smoothing），\n",
    "具体方法是在所有计数中添加一个小常量。\n",
    "用$n$表示训练集中的单词总数，用$m$表示唯一单词的数量。\n",
    "此解决方案有助于处理单元素问题，例如通过：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat{P}(x) & = \\frac{n(x) + \\epsilon_1/m}{n + \\epsilon_1}, \\\\\n",
    "    \\hat{P}(x' \\mid x) & = \\frac{n(x, x') + \\epsilon_2 \\hat{P}(x')}{n(x) + \\epsilon_2}, \\\\\n",
    "    \\hat{P}(x'' \\mid x,x') & = \\frac{n(x, x',x'') + \\epsilon_3 \\hat{P}(x'')}{n(x, x') + \\epsilon_3}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\epsilon_1,\\epsilon_2$和$\\epsilon_3$是超参数。\n",
    "以$\\epsilon_1$为例：当$\\epsilon_1 = 0$时，不应用平滑；\n",
    "当$\\epsilon_1$接近正无穷大时，$\\hat{P}(x)$接近均匀概率分布$1/m$。\n",
    "上面的公式是 :cite:`Wood.Gasthaus.Archambeau.ea.2011`\n",
    "的一个相当原始的变形。\n",
    "\n",
    "然而，这样的模型很容易变得无效，原因如下：\n",
    "首先，我们需要存储所有的计数；\n",
    "其次，这完全忽略了单词的意思。\n",
    "例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中，\n",
    "但是想根据上下文调整这类模型其实是相当困难的。\n",
    "最后，长单词序列大部分是没出现过的，\n",
    "因此一个模型如果只是简单地统计先前“看到”的单词序列频率，\n",
    "那么模型面对这种问题肯定是表现不佳的。\n",
    "\n",
    "**马尔可夫模型与$n$元语法**\n",
    "\n",
    "在讨论包含深度学习的解决方案之前，我们需要了解更多的概念和术语。\n",
    "如果$P(x_{t+1} \\mid x_t, \\ldots, x_1) = P(x_{t+1} \\mid x_t)$，\n",
    "则序列上的分布满足一阶马尔可夫性质。\n",
    "阶数越高，对应的依赖关系就越长。\n",
    "这种性质推导出了许多可以应用于序列建模的近似公式：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4),\\\\\n",
    "P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_2) P(x_4  \\mid  x_3),\\\\\n",
    "P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_1, x_2) P(x_4  \\mid  x_2, x_3).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "通常，涉及一个、两个和三个变量的概率公式分别被称为\n",
    "*一元语法*（unigram）、*二元语法*（bigram）和*三元语法*（trigram）模型。\n",
    "下面，我们将学习如何去设计更好的模型。\n",
    "\n",
    "**自然语言统计**\n",
    "\n",
    "我们看看在真实数据上如果进行自然语言统计。\n",
    "根据时光机器数据集构建词表，\n",
    "并打印前$10$个最常用的（频率最高的）单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff9b5e-1c4c-4be0-86f9-e7fe29b772e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(read_time_machine())\n",
    "corpus = [token for line in tokens for token in line]\n",
    "vocab = Vocab(corpus)\n",
    "vocab.token_freqs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fab09-aa6f-4bcb-abb9-026a88460cc9",
   "metadata": {},
   "source": [
    "正如我们所看到的，(**最流行的词**)看起来很无聊，\n",
    "这些词通常(**被称为*停用词***)（stop words），因此可以被过滤掉。\n",
    "尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。\n",
    "此外，还有个明显的问题是词频衰减的速度相当地快。\n",
    "例如，最常用单词的词频对比，第$10$个还不到第$1$个的$1/5$。\n",
    "为了更好地理解，我们可以[**画出的词频图**]：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8f3d1-cd99-44a6-b307-dca430433190",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freqs = [freq for token, freq in vocab.token_freqs]\n",
    "d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',\n",
    "         xscale='log', yscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d0c82-64c4-46f8-ba69-c895df859118",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "通过此图我们可以发现：词频以一种明确的方式迅速衰减。\n",
    "将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。\n",
    "这意味着单词的频率满足*齐普夫定律*（Zipf's law），\n",
    "即第$i$个最常用单词的频率$n_i$为：\n",
    "\n",
    "$$n_i \\propto \\frac{1}{i^\\alpha},$$\n",
    "\n",
    "等价于\n",
    "\n",
    "$$\\log n_i = -\\alpha \\log i + c,$$\n",
    "\n",
    "其中$\\alpha$是刻画分布的指数，$c$是常数。\n",
    "这告诉我们想要通过计数统计和平滑来建模单词是不可行的，\n",
    "因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。\n",
    "那么[**其他的词元组合，比如二元语法、三元语法等等，又会如何呢？**]\n",
    "我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c70ba1-875d-4d6e-9fd0-8bb97e6f6ba0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 二元语法\n",
    "bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]\n",
    "bigram_vocab = d2l.Vocab(bigram_tokens)\n",
    "\n",
    "# 三元语法\n",
    "trigram_tokens = [triple for triple in zip(\n",
    "    corpus[:-2], corpus[1:-1], corpus[2:])]\n",
    "trigram_vocab = d2l.Vocab(trigram_tokens)\n",
    "\n",
    "bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\n",
    "trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\n",
    "d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',\n",
    "         ylabel='frequency: n(x)', xscale='log', yscale='log',\n",
    "         legend=['unigram', 'bigram', 'trigram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786dd5c-6cb7-4838-bf55-32a1c98fe9b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "这张图非常令人振奋！原因有很多：\n",
    "\n",
    "1. 除了一元语法词，单词序列似乎也遵循齐普夫定律\n",
    "2. 词表中$n$元组的数量并没有那么大，这说明语言中存在相当多的结构，\n",
    "这些结构给了我们应用模型的希望；\n",
    "3. 很多$n$元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。\n",
    "作为代替，我们将使用基于深度学习的模型。\n",
    "\n",
    "**读取长序列数据**\n",
    "\n",
    "由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。\n",
    "\n",
    "当序列变得太长而不能被模型一次性全部处理时，\n",
    "我们可能希望拆分这样的序列方便模型读取。\n",
    "\n",
    "在介绍该模型之前，我们看一下总体策略。\n",
    "假设我们将使用神经网络来训练语言模型，\n",
    "模型中的网络一次处理具有预定义长度\n",
    "（例如$n$个时间步）的一个小批量序列。\n",
    "现在的问题是如何[**随机生成一个小批量数据的特征和标签以供读取。**]\n",
    "\n",
    "首先，由于文本序列可以是任意长的，\n",
    "例如整本《时光机器》（*The Time Machine*），\n",
    "于是任意长的序列可以被我们划分为具有相同时间步数的子序列。\n",
    "当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。\n",
    "假设网络一次只处理具有$n$个时间步的子序列。\n",
    "\n",
    "从原始文本序列获得子序列的所有不同的方式，\n",
    "其中$n=5$，并且每个时间步的词元对应于一个字符。\n",
    "请注意，因为我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。\n",
    "\n",
    "![分割文本时，不同的偏移量会导致不同的子序列](../img/timemachine-5gram.svg)\n",
    "\n",
    "因此，我们应该选择哪一个呢？\n",
    "事实上，他们都一样的好。\n",
    "然而，如果我们只选择一个偏移量，\n",
    "那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。\n",
    "因此，我们可以从随机偏移量开始划分序列，\n",
    "以同时获得*覆盖性*（coverage）和*随机性*（randomness）。\n",
    "下面，我们将描述如何实现*随机采样*（random sampling）和\n",
    "*顺序分区*（sequential partitioning）策略。\n",
    "\n",
    "**随机采样**\n",
    "\n",
    "(**在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。**)\n",
    "在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。\n",
    "对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，\n",
    "因此标签是移位了一个词元的原始序列。\n",
    "\n",
    "下面的代码每次可以从数据中随机生成一个小批量。\n",
    "在这里，参数`batch_size`指定了每个小批量中子序列样本的数目，\n",
    "参数`num_steps`是每个子序列中预定义的时间步数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b22e9-de86-46ba-9f74-984671f29a9e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053d8c7-451c-48ef-9358-57ca77dfa415",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "下面我们[**生成一个从$0$到$34$的序列**]。假设批量大小为$2$，时间步数为$5$，这意味着可以生成\n",
    "$\\lfloor (35 - 1) / 5 \\rfloor= 6$个“特征－标签”子序列对。\n",
    "如果设置小批量大小为$2$，我们只能得到$3$个小批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084e722-0e43-4f79-8e31-a01f14364b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0786a-0911-44a9-abb8-43a1d5d59754",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 5. 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02cb23-86fe-43b4-a620-879da0710524",
   "metadata": {},
   "source": [
    "## 5.1 网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35405a64-cfa3-4c7e-8754-018f141adda1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "$n$元语法模型，其中单词$x_t$在时间步$t$的条件概率仅取决于前面$n-1$个单词。\n",
    "对于时间步$t-(n-1)$之前的单词，\n",
    "如果我们想将其可能产生的影响合并到$x_t$上，\n",
    "需要增加$n$，然而模型参数的数量也会随之呈指数增长，\n",
    "因为词表$\\mathcal{V}$需要存储$|\\mathcal{V}|^n$个数字，\n",
    "因此与其将$P(x_t \\mid x_{t-1}, \\ldots, x_{t-n+1})$模型化，\n",
    "不如使用隐变量模型：\n",
    "\n",
    "$$P(x_t \\mid x_{t-1}, \\ldots, x_1) \\approx P(x_t \\mid h_{t-1}),$$\n",
    "\n",
    "其中$h_{t-1}$是*隐状态*（hidden state），\n",
    "也称为*隐藏变量*（hidden variable），\n",
    "它存储了到时间步$t-1$的序列信息。\n",
    "通常，我们可以基于当前输入$x_{t}$和先前隐状态$h_{t-1}$\n",
    "来计算时间步$t$处的任何时间的隐状态：\n",
    "\n",
    "$$h_t = f(x_{t}, h_{t-1}).$$\n",
    "\n",
    "对于函数$f$，隐变量模型不是近似值。\n",
    "毕竟$h_t$是可以仅仅存储到目前为止观察到的所有数据，\n",
    "然而这样的操作可能会使计算和存储的代价都变得昂贵。\n",
    "\n",
    "值得注意的是，隐藏层和隐状态指的是两个截然不同的概念。\n",
    "如上所述，隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层，\n",
    "而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的*输入*，\n",
    "并且这些状态只能通过先前时间步的数据来计算。\n",
    "\n",
    "*循环神经网络*（recurrent neural networks，RNNs）\n",
    "是具有隐状态的神经网络。\n",
    "\n",
    "**无隐状态的神经网络**\n",
    "\n",
    "让我们来看一看只有单隐藏层的多层感知机。\n",
    "设隐藏层的激活函数为$\\phi$，\n",
    "给定一个小批量样本$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，\n",
    "其中批量大小为$n$，输入维度为$d$，\n",
    "则隐藏层的输出$\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$通过下式计算：\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h).$$\n",
    "\n",
    "隐藏层权重参数为$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$，\n",
    "偏置参数为$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$，\n",
    "以及隐藏单元的数目为$h$。\n",
    "因此求和时可以应用广播机制。\n",
    "接下来，将隐藏变量$\\mathbf{H}$用作输出层的输入。\n",
    "输出层由下式给出：\n",
    "\n",
    "$$\\mathbf{O} = \\mathbf{H} \\mathbf{W}_{hq} + \\mathbf{b}_q,$$\n",
    "\n",
    "其中，$\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$是输出变量，\n",
    "$\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$是权重参数，\n",
    "$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$是输出层的偏置参数。\n",
    "如果是分类问题，我们可以用$\\text{softmax}(\\mathbf{O})$\n",
    "来计算输出类别的概率分布。\n",
    "\n",
    "只要可以随机选择“特征-标签”对，\n",
    "并且通过自动微分和随机梯度下降能够学习网络参数就可以了。\n",
    "\n",
    "**有隐状态的循环神经网络**\n",
    "\n",
    "有了隐状态后，情况就完全不同了。\n",
    "假设我们在时间步$t$有小批量输入$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$。\n",
    "换言之，对于$n$个序列样本的小批量，\n",
    "$\\mathbf{X}_t$的每一行对应于来自该序列的时间步$t$处的一个样本。\n",
    "接下来，用$\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$\n",
    "表示时间步$t$的隐藏变量。\n",
    "与多层感知机不同的是，\n",
    "我们在这里保存了前一个时间步的隐藏变量$\\mathbf{H}_{t-1}$，\n",
    "并引入了一个新的权重参数$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，\n",
    "来描述如何在当前时间步中使用前一个时间步的隐藏变量。\n",
    "具体地说，当前时间步隐藏变量由当前时间步的输入\n",
    "与前一个时间步的隐藏变量一起计算得出：\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h).$$\n",
    "\n",
    "多添加了一项\n",
    "$\\mathbf{H}_{t-1} \\mathbf{W}_{hh}$\n",
    "\n",
    "从相邻时间步的隐藏变量$\\mathbf{H}_t$和\n",
    "$\\mathbf{H}_{t-1}$之间的关系可知，\n",
    "这些变量捕获并保留了序列直到其当前时间步的历史信息，\n",
    "就如当前时间步下神经网络的状态或记忆，\n",
    "因此这样的隐藏变量被称为*隐状态*（hidden state）。\n",
    "由于在当前时间步中，\n",
    "隐状态使用的定义与前一个时间步中使用的定义相同，\n",
    "因此计算是*循环的*（recurrent）。\n",
    "于是基于循环计算的隐状态神经网络被命名为\n",
    "*循环神经网络*（recurrent neural network）。\n",
    "在循环神经网络中执行计算的层\n",
    "称为*循环层*（recurrent layer）。\n",
    "\n",
    "对于时间步$t$，输出层的输出类似于多层感知机中的计算：\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "循环神经网络的参数包括隐藏层的权重\n",
    "$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$和偏置$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$，\n",
    "以及输出层的权重$\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$\n",
    "和偏置$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$。\n",
    "值得一提的是，即使在不同的时间步，循环神经网络也总是使用这些模型参数。\n",
    "因此，循环神经网络的参数开销不会随着时间步的增加而增加。\n",
    "\n",
    "在任意时间步$t$，隐状态的计算可以被视为：\n",
    "\n",
    "1. 拼接当前时间步$t$的输入$\\mathbf{X}_t$和前一时间步$t-1$的隐状态$\\mathbf{H}_{t-1}$；\n",
    "1. 将拼接的结果送入带有激活函数$\\phi$的全连接层。\n",
    "   全连接层的输出是当前时间步$t$的隐状态$\\mathbf{H}_t$。\n",
    "   \n",
    "在本例中，模型参数是$\\mathbf{W}_{xh}$和$\\mathbf{W}_{hh}$的拼接，\n",
    "以及$\\mathbf{b}_h$的偏置，所有这些参数都来自 :eqref:`rnn_h_with_state`。\n",
    "当前时间步$t$的隐状态$\\mathbf{H}_t$\n",
    "将参与计算下一时间步$t+1$的隐状态$\\mathbf{H}_{t+1}$。\n",
    "而且$\\mathbf{H}_t$还将送入全连接输出层，\n",
    "用于计算当前时间步$t$的输出$\\mathbf{O}_t$。\n",
    "\n",
    "![具有隐状态的循环神经网络](../img/rnn.svg)\n",
    "\n",
    "我们刚才提到，隐状态中\n",
    "$\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}$的计算，\n",
    "相当于$\\mathbf{X}_t$和$\\mathbf{H}_{t-1}$的拼接\n",
    "与$\\mathbf{W}_{xh}$和$\\mathbf{W}_{hh}$的拼接的矩阵乘法。\n",
    "虽然这个性质可以通过数学证明，\n",
    "但在下面我们使用一个简单的代码来说明一下。\n",
    "首先，我们定义矩阵`X`、`W_xh`、`H`和`W_hh`，\n",
    "它们的形状分别为$(3，1)$、$(1，4)$、$(3，4)$和$(4，4)$。\n",
    "分别将`X`乘以`W_xh`，将`H`乘以`W_hh`，\n",
    "然后将这两个乘法相加，我们得到一个形状为$(3，4)$的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1227ef-f526-442d-bae2-48a9f14b3000",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))\n",
    "H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))\n",
    "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa37e2c-5781-4ff7-be57-ff3d7ebed6ff",
   "metadata": {},
   "source": [
    "## 5.2 基于循环神经网络的字符级语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d172569-e955-4f43-896a-f5bff37701a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "语言模型中，我们的目标是根据过去的和当前的词元预测下一个词元，\n",
    "因此我们将原始序列移位一个词元作为标签。\n",
    "Bengio等人首先提出使用神经网络进行语言建模\n",
    "接下来，我们看一下如何使用循环神经网络来构建语言模型。\n",
    "设小批量大小为1，批量中的文本序列为“machine”。\n",
    "为了简化后续部分的训练，我们考虑使用\n",
    "*字符级语言模型*（character-level language model），\n",
    "将文本词元化为字符而不是单词。\n",
    "\n",
    "下图演示了如何通过基于字符级语言建模的循环神经网络，\n",
    "使用当前的和先前的字符预测下一个字符。\n",
    "\n",
    "![基于循环神经网络的字符级语言模型：输入序列和标签序列分别为“machin”和“achine”](../img/rnn-train.svg)\n",
    "\n",
    "在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，\n",
    "然后利用交叉熵损失计算模型输出和标签之间的误差。\n",
    "由于隐藏层中隐状态的循环计算，第$3$个时间步的输出$\\mathbf{O}_3$\n",
    "由文本序列“m”“a”和“c”确定。\n",
    "由于训练数据中这个文本序列的下一个字符是“h”，\n",
    "因此第$3$个时间步的损失将取决于下一个字符的概率分布，\n",
    "而下一个字符是基于特征序列“m”“a”“c”和这个时间步的标签“h”生成的。\n",
    "\n",
    "在实践中，我们使用的批量大小为$n>1$，\n",
    "每个词元都由一个$d$维向量表示。\n",
    "因此，在时间步$t$输入$\\mathbf X_t$将是一个$n\\times d$矩阵。\n",
    "\n",
    "**困惑度（Perplexity）**\n",
    "\n",
    "最后，让我们讨论如何度量语言模型的质量，\n",
    "这将在后续部分中用于评估基于循环神经网络的模型。\n",
    "一个好的语言模型能够用高度准确的词元来预测我们接下来会看到什么。\n",
    "考虑一下由不同的语言模型给出的对“It is raining ...”（“...下雨了”）的续写：\n",
    "\n",
    "1. \"It is raining outside\"（外面下雨了）；\n",
    "1. \"It is raining banana tree\"（香蕉树下雨了）；\n",
    "1. \"It is raining piouw;kcj pwepoiut\"（piouw;kcj pwepoiut下雨了）。\n",
    "\n",
    "就质量而言，例$1$显然是最合乎情理、在逻辑上最连贯的。\n",
    "虽然这个模型可能没有很准确地反映出后续词的语义，\n",
    "比如，“It is raining in San Francisco”（旧金山下雨了）\n",
    "和“It is raining in winter”（冬天下雨了）\n",
    "可能才是更完美的合理扩展，\n",
    "但该模型已经能够捕捉到跟在后面的是哪类单词。\n",
    "例$2$则要糟糕得多，因为其产生了一个无意义的续写。\n",
    "尽管如此，至少该模型已经学会了如何拼写单词，\n",
    "以及单词之间的某种程度的相关性。\n",
    "最后，例$3$表明了训练不足的模型是无法正确地拟合数据的。\n",
    "\n",
    "我们可以通过计算序列的似然概率来度量模型的质量。\n",
    "然而这是一个难以理解、难以比较的数字。\n",
    "毕竟，较短的序列比较长的序列更有可能出现，\n",
    "因此评估模型产生托尔斯泰的巨著《战争与和平》的可能性\n",
    "不可避免地会比产生圣埃克苏佩里的中篇小说《小王子》可能性要小得多。\n",
    "而缺少的可能性值相当于平均数。\n",
    "\n",
    "在这里，信息论可以派上用场了。\n",
    "我们在引入softmax回归时定义了熵、惊异和交叉熵，\n",
    "如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。\n",
    "一个更好的语言模型应该能让我们更准确地预测下一个词元。\n",
    "因此，它应该允许我们在压缩序列时花费更少的比特。\n",
    "所以我们可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量：\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1),$$\n",
    "\n",
    "其中$P$由语言模型给出，\n",
    "$x_t$是在时间步$t$从该序列中观察到的实际词元。\n",
    "这使得不同长度的文档的性能具有了可比性。\n",
    "由于历史原因，自然语言处理的科学家更喜欢使用一个叫做*困惑度*（perplexity）的量。\n",
    "\n",
    "$$\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right).$$\n",
    "\n",
    "困惑度的最好的理解是“下一个词元的实际选择数的调和平均数”。\n",
    "我们看看一些案例。\n",
    "\n",
    "* 在最好的情况下，模型总是完美地估计标签词元的概率为1。\n",
    "  在这种情况下，模型的困惑度为1。\n",
    "* 在最坏的情况下，模型总是预测标签词元的概率为0。\n",
    "  在这种情况下，困惑度是正无穷大。\n",
    "* 在基线上，该模型的预测是词表的所有可用词元上的均匀分布。\n",
    "  在这种情况下，困惑度等于词表中唯一词元的数量。\n",
    "  事实上，如果我们在没有任何压缩的情况下存储序列，\n",
    "  这将是我们能做的最好的编码方式。\n",
    "  因此，这种方式提供了一个重要的上限，\n",
    "  而任何实际模型都必须超越这个上限。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c201356-d309-425a-a3cc-9477d4ada2bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 5. 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b5b7b-725b-42b4-ad9a-306cb40c7f72",
   "metadata": {},
   "source": [
    "## 5.1 整合dataloader相关代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f66d83e-0957-4466-8225-5c9ce2a3760b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class TimeMachineVocab:\n",
    "    def __init__(self, tokens):\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(set(tokens))}\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "def read_time_machine():\n",
    "    with open('../data/timemachine.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "def tokenize(lines, mode='char'):\n",
    "    # Tokenize the text into characters\n",
    "    return [list(line) for line in lines]\n",
    "\n",
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    # Flatten the list of tokens and create a vocabulary\n",
    "    tokens_flat = [token for line in tokens for token in line]\n",
    "    vocab = TimeMachineVocab(tokens_flat)\n",
    "    # Convert text to token indices\n",
    "    corpus = [vocab[token] for token in tokens_flat]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "class TimeMachineDataset(Dataset):\n",
    "    def __init__(self, corpus, num_steps):\n",
    "        self.corpus = corpus\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus) // self.num_steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.num_steps\n",
    "        X = self.corpus[i:i + self.num_steps]\n",
    "        Y = self.corpus[i + 1:i + 1 + self.num_steps]\n",
    "        return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X_batch, Y_batch = zip(*batch)\n",
    "    return torch.stack(X_batch), torch.stack(Y_batch)\n",
    "\n",
    "# Main function to create DataLoader\n",
    "def create_dataloader(batch_size, num_steps, max_tokens):\n",
    "    corpus, vocab = load_corpus_time_machine(max_tokens)\n",
    "    dataset = TimeMachineDataset(corpus, num_steps)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    return dataloader, vocab\n",
    "\n",
    "# Example usage\n",
    "# batch_size, num_steps, max_tokens = 2, 5, 50\n",
    "# dataloader, vocab = create_dataloader(batch_size, num_steps, max_tokens)\n",
    "\n",
    "# for X, Y in dataloader:\n",
    "#     print(\"X:\", X)\n",
    "#     print(\"Y:\", Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73b5cde-ccd4-407d-8060-da3157929334",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 5.2 训练一个char级别的语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f693522-597a-450e-a12c-60fac754608d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import d2l\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "\n",
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens)\n",
    "\n",
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape\n",
    "\n",
    "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e0193-f670-40bf-86e2-bfed7a96d7e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c5305-a561-4cf0-b78c-f17dcaf8b3fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始化网络，查看预测效果\n",
    "\n",
    "device = d2l.try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(device)\n",
    "d2l.predict_ch8('time traveller', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0706b-3195-4fac-8a01-7b1fb60516f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs, lr = 500, 0.2\n",
    "d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e4665-e7f1-486d-83b9-357bcb0d244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练之后的预测\n",
    "\n",
    "d2l.predict_ch8('time traveller', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f80b1b-ea6c-46f3-859d-002ca14c4c7e",
   "metadata": {},
   "source": [
    "# 6. 练习 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23062c5b-dff5-4467-be1b-90e1011b3116",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- 对于time machine，进行word级别的tokenize，使用rnn网络进行训练，并验证语言模型的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf6654-2e5c-43d7-82b0-bd4ab36f0009",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
