{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2b31d80-1d82-48d9-a752-d3c6d4ea9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import Embedding\n",
    "from torch.nn import Linear\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "505303ee-06d5-43c6-aca8-7c742867d488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1214a7d30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313fba9d-4610-4044-8f5e-44103776e20d",
   "metadata": {},
   "source": [
    "### 1.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e02c7f-9fa8-4a39-bca7-6bff81e653c2",
   "metadata": {},
   "source": [
    "#### 1.1 Embedding的作用\n",
    "\n",
    "Embedding层的作用是将有限集合中的元素，转变成指定size的向量。这个有限集合可以使NLP中的词汇表，可以使分类任务中的label，当然无论是什么，最终都要以元素索引传递给Embedding。例如，将包含3个元素的词汇表W={'优', '良', '差'}中的每个元素转换为5维向量。如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30314121-d9b6-4d90-886a-fd09ec19651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先定义一个Embedding层：\n",
    "emb = Embedding(num_embeddings=3, embedding_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43565d4c-718c-49d4-aa70-e35da4a55762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(3, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d9b6763-fc3a-4e78-996d-c793baff6163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换第一个元素\n",
    "emb(torch.tensor([0],dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d864947-fb39-40c1-acbe-46eba5e2dd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换第二个元素\n",
    "emb(torch.tensor([1],dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0ba202e-2e81-4054-a993-8d0649f1964e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8923, -0.0583, -0.1955, -0.9656,  0.4224]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换第三个元素\n",
    "emb(torch.tensor([2],dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d333c62-803b-4849-894b-0584f4b1d1ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 如果超出词库规模，就会产生异常错误：\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 转换第四个元素\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43memb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# 如果超出词库规模，就会产生异常错误：\n",
    "# 转换第四个元素\n",
    "emb(torch.tensor([3],dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d2f19-a1e0-4640-b286-ab047c0d64a4",
   "metadata": {},
   "source": [
    "初始时，所有向量表示都是随机的，但却并非一成不变的，例如在NLP任务中，随着网络的训练，表示'优'与'良'的两个向量相似度会逐渐减小，而表示'优'与'差'的两个向量相似度会逐渐增大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df62ce-5c30-4b37-b038-8947c49af612",
   "metadata": {},
   "source": [
    "#### 1.2 Embedding的用法\n",
    "\n",
    "接下来我们详细说说pytorch中Embedding层的使用方法。Embedding类主要参数如下：\n",
    "\n",
    "num_embeddings (int) - 嵌入字典的大小，即共有多少个元素需要转换\n",
    "\n",
    "embedding_dim (int) - 每个嵌入向量的大小，即转换后获得向量的size\n",
    "\n",
    "padding_idx (int, optional) - 如果提供的话，输出遇到此下标时用零填充\n",
    "\n",
    "max_norm (float, optional) - 如果提供的话，会重新归一化词嵌入，使它们的范数小于提供的值\n",
    "\n",
    "norm_type (float, optional) - 对于max_norm选项计算p范数时的p\n",
    "\n",
    "scale_grad_by_freq (boolean, optional) - 如果提供的话，会根据字典中单词频率缩放梯度\n",
    "\n",
    "weight weight (Tensor) -形状为(num_embeddings, embedding_dim)的模块中可学习的权值\n",
    "\n",
    " \n",
    "Embedding是怎么实现的呢？其实，在初始化Embedding层时，Embedding会根据默认随机初始化num_embeddings * embedding_dim的正态分布的权重。以上面例子为例，我们看看它的参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6869391-6b92-4a4a-a91e-6342e51992b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.8793, -0.0721,  0.1578, -0.7735,  0.1991],\n",
       "        [ 0.0457,  0.1530, -0.4757, -0.1110,  0.2927],\n",
       "        [-0.1578, -0.0288,  2.3571, -1.0373,  1.5748],\n",
       "        [-0.6298, -0.9274,  0.5451,  0.0663, -0.4370],\n",
       "        [ 0.7626,  0.4415,  1.1651,  2.0154,  0.1374],\n",
       "        [ 0.9386, -0.1860, -0.6446,  1.5392, -0.8696],\n",
       "        [-3.3312, -0.7479, -0.0255, -1.0233, -0.6540],\n",
       "        [ 0.7317, -1.4344, -0.5008,  0.1716, -0.1600],\n",
       "        [-0.5047, -1.4746, -1.0412,  0.7323, -1.0483],\n",
       "        [-0.4709,  0.2911,  1.9907, -0.9247, -0.9301]], requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先定义一个Embedding层：\n",
    "emb = Embedding(num_embeddings=10, embedding_dim=5)\n",
    "emb.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326814ba-dd8d-4ad3-9b1e-047395bb2762",
   "metadata": {},
   "source": [
    "仔细观察这些权重值，每一行都与上方{'优', '良', '差'}对应。当我们在emb中输入张量torch.tensor([0])时，输出了第一行，当我们在emb中输入张量torch.tensor([1])时，输出了第二行。所以，我们可以猜测，Embedding的工作原理就是初始化一个指定shape的矩阵，在进行转换是，根据输入的tensor值，索引矩阵的行。确实如此，Embedding源码就是这么做的。\n",
    "\n",
    "当然，Embedding的权重参数也不一定非得随机初始化，也可以手动指定。如下所示，我们先手动初始化一个3 * 5的矩阵，然后将其作为Embedding的权重参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15b10753-2754-4b2f-baaa-1dc60901d591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2873, 0.3486, 0.9579, 0.4075, 0.7819],\n",
       "        [0.7165, 0.1768, 0.0748, 0.9799, 0.5261],\n",
       "        [0.8427, 0.6036, 0.6608, 0.8735, 0.9741]], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机初始化一个3 * 5 的矩阵\n",
    "emb_weight = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "# 这里需要注意，手动初始化参数时，最好设置requires_grad=True，后续训练时才能更新权重。\n",
    "emb_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d505d74-8b88-4934-9c94-b93ecd573a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过这个预先定义的矩阵，初始化Embedding层\n",
    "emb2 = Embedding.from_pretrained(emb_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "913920ba-06ea-4415-b701-bf2db3fc9eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2873, 0.3486, 0.9579, 0.4075, 0.7819],\n",
       "        [0.7165, 0.1768, 0.0748, 0.9799, 0.5261],\n",
       "        [0.8427, 0.6036, 0.6608, 0.8735, 0.9741]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f92ebb7-7136-4871-9750-ccdd6e456336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2873, 0.3486, 0.9579, 0.4075, 0.7819]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换第一个元素\n",
    "emb2(torch.tensor([0],dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98e2a724-46ec-4bc1-a2c8-b17ec17e600a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2873, 0.3486, 0.9579, 0.4075, 0.7819],\n",
       "        [0.7165, 0.1768, 0.0748, 0.9799, 0.5261],\n",
       "        [0.8427, 0.6036, 0.6608, 0.8735, 0.9741]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看所有权重参数\n",
    "emb2.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67062c67-4859-4a01-9653-ebee8acb5a80",
   "metadata": {},
   "source": [
    "这种手动指定参数参数话Embedding层的方式在迁移学习中非常实用，例如在NLP任务中，我们可以使用开源的词向量模型进行初始化，使得我们的模型更快收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa5dc9-01e1-4abf-ad13-a01ad6a2c301",
   "metadata": {},
   "source": [
    "### 2 Linear层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b36c2-e079-41a0-b5c4-cd51165328d8",
   "metadata": {},
   "source": [
    "#### 2.1 Linear层的作用\n",
    "Linear层是最古老、最基础的一种网络结构了，作用是对输入向量进行线性变换，输出指定size的向量。例如，将size为3的向量，转为size为5的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e0c031b-60f5-423a-bf84-47b474b4b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个Linear层\n",
    "lin = Linear(in_features=3, out_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6cb18b7-2666-4845-80d0-e6c2f5859cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8313, 0.8116, 0.8553])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机初始化一个size为3的向量\n",
    "x = torch.rand(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc83d10d-76d3-4820-abe6-62b80278da50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f4a9619-4083-40cb-9751-915cf09680e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4413,  0.8174, -0.9255,  0.5708, -0.3130], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经Linear层进行转换\n",
    "y = lin(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65fb30b8-e321-40e2-8968-69f0bfa33fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a845d701-1930-4680-9a0d-8970fd493a23",
   "metadata": {},
   "source": [
    "#### 2.2 Linear的用法\n",
    " \n",
    "Linear类就3个参数：\n",
    "\n",
    "in_features：指的是输入张量的size\n",
    "out_features：指的是输出张量的size\n",
    "bias：是否使用偏置，默认为True，表示使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17617626-ee83-40c3-8bfe-6a5ab2e97f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3831,  0.0722,  0.4309],\n",
       "        [ 0.4183,  0.3587, -0.4178],\n",
       "        [-0.4158, -0.3492,  0.0725],\n",
       "        [ 0.5754, -0.3647,  0.3077],\n",
       "        [-0.3196, -0.5428, -0.1227]], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = lin.weight\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "652d2cc5-d174-4d18-a3b6-8131462b4804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.3327,  0.5360, -0.3586,  0.1253,  0.4982], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = lin.bias\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfbc46b7-6d84-43c3-8e0a-c778a5736635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4413,  0.8174, -0.9255,  0.5708, -0.3130], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.matmul(w.T) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbce5eac-3660-42dd-a32a-778b5ffe30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear层的参数也可以进行手动修改，不过必须转为Parameter\n",
    "\n",
    "lin_weight = torch.rand(3, 5, requires_grad=True)\n",
    "lin_weight\n",
    "\n",
    "from torch.nn import Parameter\n",
    "lin.weight = Parameter(lin_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09d6b4-eeab-4139-a7b3-d88ee63a4923",
   "metadata": {},
   "source": [
    "### 3.Embedding与Linear的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd081f-2294-4de3-a767-d3707b4b2db4",
   "metadata": {},
   "source": [
    "- Embedding只针对数据集规模有限的离散型数据，Linear即可用于离散型数据，也可用于连续型数据，且对数据集规模无限制。对于Embedding能实现的功能，Liner都能实现，只不过需要先进性一次手动one-hot编码。\n",
    "\n",
    "- Embedding本质是通过元素的索引，获取矩阵对应行作为输出，而Linear本质是矩阵相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb857c-be41-40e0-8151-696e421d89e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d45387c-3644-4f8e-baf6-ae6a5a772537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43961b61-6c37-4b56-8c2d-cf0306c666e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
